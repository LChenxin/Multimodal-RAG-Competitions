{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b152f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import hashlib\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import random\n",
    "\n",
    "from get_text_embedding import get_text_embedding\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add01a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageChunkLoader:\n",
    "    def __init__(self, json_path: str):\n",
    "        self.json_path = json_path\n",
    "    def load_chunks(self) -> List[Dict[str, Any]]:\n",
    "        with open(self.json_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afbb00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel:\n",
    "    def __init__(self, batch_size: int = 64):\n",
    "        self.api_key = os.getenv('LOCAL_API_KEY')\n",
    "        self.base_url = os.getenv('LOCAL_BASE_URL')\n",
    "        self.embedding_model = os.getenv('LOCAL_EMBEDDING_MODEL')\n",
    "        self.batch_size = batch_size\n",
    "        if not self.api_key or not self.base_url:\n",
    "            raise ValueError('请在.env中配置LOCAL_API_KEY和LOCAL_BASE_URL')\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        return get_text_embedding(\n",
    "            texts,\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            embedding_model=self.embedding_model,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        return self.embed_texts([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e741cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.embeddings = []\n",
    "        self.chunks = []\n",
    "    def add_chunks(self, chunks: List[Dict[str, Any]], embeddings: List[List[float]]):\n",
    "        self.chunks.extend(chunks)\n",
    "        self.embeddings.extend(embeddings)\n",
    "    def search(self, query_embedding: List[float], top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        from numpy import dot\n",
    "        from numpy.linalg import norm\n",
    "        import numpy as np\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "        emb_matrix = np.array(self.embeddings)\n",
    "        query_emb = np.array(query_embedding)\n",
    "        sims = emb_matrix @ query_emb / (norm(emb_matrix, axis=1) * norm(query_emb) + 1e-8)\n",
    "        idxs = sims.argsort()[::-1][:top_k]\n",
    "        return [self.chunks[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b79cfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    def __init__(self, chunk_json_path: str, model_path: str = None, batch_size: int = 32):\n",
    "        self.loader = PageChunkLoader(chunk_json_path)\n",
    "        self.embedding_model = EmbeddingModel(batch_size=batch_size)\n",
    "        self.vector_store = SimpleVectorStore()\n",
    "    def setup(self):\n",
    "        print(\"加载所有页chunk...\")\n",
    "        chunks = self.loader.load_chunks()\n",
    "        print(f\"共加载 {len(chunks)} 个chunk\")\n",
    "        print(\"生成嵌入...\")\n",
    "        embeddings = self.embedding_model.embed_texts([c['content'] for c in chunks])\n",
    "        print(\"存储向量...\")\n",
    "        self.vector_store.add_chunks(chunks, embeddings)\n",
    "        print(\"RAG向量库构建完成！\")\n",
    "    def query(self, question: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "        q_emb = self.embedding_model.embed_text(question)\n",
    "        results = self.vector_store.search(q_emb, top_k)\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"chunks\": results\n",
    "        }\n",
    "\n",
    "    def generate_answer(self, question: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        检索+大模型生成式回答，返回结构化结果\n",
    "        \"\"\"\n",
    "        qwen_api_key = os.getenv('LOCAL_API_KEY')\n",
    "        qwen_base_url = os.getenv('LOCAL_BASE_URL')\n",
    "        qwen_model = os.getenv('LOCAL_TEXT_MODEL')\n",
    "        if not qwen_api_key or not qwen_base_url or not qwen_model:\n",
    "            raise ValueError('请在.env中配置LOCAL_API_KEY、LOCAL_BASE_URL、LOCAL_TEXT_MODEL')\n",
    "        q_emb = self.embedding_model.embed_text(question)\n",
    "        chunks = self.vector_store.search(q_emb, top_k)\n",
    "        # 拼接检索内容，带上元数据\n",
    "        context = \"\\n\".join([\n",
    "            f\"[文件名]{c['metadata']['file_name']} [页码]{c['metadata']['page']}\\n{c['content']}\" for c in chunks\n",
    "        ])\n",
    "        # 明确要求输出JSON格式 answer/page/filename\n",
    "        prompt = (\n",
    "            f\"你是一名专业的金融分析助手，请根据以下检索到的内容回答用户问题。\\n\"\n",
    "            f\"请严格按照如下JSON格式输出：\\n\"\n",
    "            f'{{\"answer\": \"你的简洁回答\", \"filename\": \"来源文件名\", \"page\": \"来源页码\"}}'\"\\n\"\n",
    "            f\"检索内容：\\n{context}\\n\\n问题：{question}\\n\"\n",
    "            f\"请确保输出内容为合法JSON字符串，不要输出多余内容。\"\n",
    "        )\n",
    "        client = OpenAI(api_key=qwen_api_key, base_url=qwen_base_url)\n",
    "        completion = client.chat.completions.create(\n",
    "            model=qwen_model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"你是一名专业的金融分析助手。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            max_tokens=1024\n",
    "        )\n",
    "        \n",
    "        import json as pyjson\n",
    "        from extract_json_array import extract_json_array\n",
    "        raw = completion.choices[0].message.content.strip()\n",
    "        # 用 extract_json_array 提取 JSON 对象\n",
    "        json_str = extract_json_array(raw, mode='objects')\n",
    "        if json_str:\n",
    "            try:\n",
    "                arr = pyjson.loads(json_str)\n",
    "                # 只取第一个对象\n",
    "                if isinstance(arr, list) and arr:\n",
    "                    j = arr[0]\n",
    "                    answer = j.get('answer', '')\n",
    "                    filename = j.get('filename', '')\n",
    "                    page = j.get('page', '')\n",
    "                else:\n",
    "                    answer = raw\n",
    "                    filename = chunks[0]['metadata']['file_name'] if chunks else ''\n",
    "                    page = chunks[0]['metadata']['page'] if chunks else ''\n",
    "            except Exception:\n",
    "                answer = raw\n",
    "                filename = chunks[0]['metadata']['file_name'] if chunks else ''\n",
    "                page = chunks[0]['metadata']['page'] if chunks else ''\n",
    "        else:\n",
    "            answer = raw\n",
    "            filename = chunks[0]['metadata']['file_name'] if chunks else ''\n",
    "            page = chunks[0]['metadata']['page'] if chunks else ''\n",
    "        # 结构化输出\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"filename\": filename,\n",
    "            \"page\": page,\n",
    "            \"retrieval_chunks\": chunks\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04e17890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Dir: d:\\Datawhale\\Multimodal-RAG-Competitions\\notebook\n",
      "Project Root : d:\\Datawhale\\Multimodal-RAG-Competitions\n",
      "Train JSON   : d:\\Datawhale\\Multimodal-RAG-Competitions\\data\\train.json\n",
      "Chunks JSON  : d:\\Datawhale\\Multimodal-RAG-Competitions\\notebook\\sample_pdf_page_chunks.json\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports & Paths\n",
    "from pathlib import Path\n",
    "import os, json, random\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "import math\n",
    "\n",
    "# Notebook is in .../notebook; project root is parent\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJ_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Try common locations for train.json\n",
    "CANDIDATE_TRAIN = [\n",
    "    PROJ_ROOT / \"datas\" / \"train.json\",\n",
    "    PROJ_ROOT / \"data\" / \"train.json\",\n",
    "    NOTEBOOK_DIR / \"datas\" / \"train.json\",\n",
    "    NOTEBOOK_DIR / \"data\" / \"train.json\",\n",
    "]\n",
    "TRAIN_PATH = next((p for p in CANDIDATE_TRAIN if p.exists()), None)\n",
    "if TRAIN_PATH is None:\n",
    "    raise FileNotFoundError(f\"train.json not found in: {CANDIDATE_TRAIN}\")\n",
    "\n",
    "# Chunk JSON path (your earlier structure)\n",
    "CHUNK_JSON_PATH = PROJ_ROOT / \"notebook\" / \"sample_pdf_page_chunks.json\"\n",
    "\n",
    "# Outputs\n",
    "EVAL_RAW_PATH = PROJ_ROOT / \"eval_train_raw.json\"\n",
    "EVAL_SUMMARY_PATH = PROJ_ROOT / \"eval_train_scored.json\"\n",
    "\n",
    "print(\"Notebook Dir:\", NOTEBOOK_DIR)\n",
    "print(\"Project Root :\", PROJ_ROOT)\n",
    "print(\"Train JSON   :\", TRAIN_PATH)\n",
    "print(\"Chunks JSON  :\", CHUNK_JSON_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "84c706ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载所有页chunk...\n",
      "共加载 7802 个chunk\n",
      "生成嵌入...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 244/244 [05:26<00:00,  1.34s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "存储向量...\n",
      "RAG向量库构建完成！\n",
      "RAG initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ⚙️ 2) Initialize RAG\n",
    "# from your_module.rag import SimpleRAG  # ← update to your actual import path\n",
    "\n",
    "try:\n",
    "    rag = SimpleRAG(str(CHUNK_JSON_PATH))\n",
    "    rag.setup()\n",
    "    print(\"RAG initialized.\")\n",
    "except NameError:\n",
    "    raise NameError(\"SimpleRAG is not defined. Import your class (e.g., `from your_module.rag import SimpleRAG`).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7135184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 118 | Sample size = 12\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3, 13, 14, 17, 28, 31, 35, 69, 81, 86]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Load train and sample\n",
    "with open(TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "N = len(train_data)\n",
    "random.seed(42)\n",
    "\n",
    "sample_size = max(1, math.ceil(N * 0.10))\n",
    "all_idx = list(range(N))\n",
    "sample_idx = sorted(random.sample(all_idx, sample_size)) if sample_size < N else all_idx\n",
    "\n",
    "print(f\"Train size = {N} | Sample size = {len(sample_idx)}\")\n",
    "sample_idx[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e777c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Jaccard helper\n",
    "def jaccard_char(a: str, b: str) -> float:\n",
    "    a = (a or \"\").strip()\n",
    "    b = (b or \"\").strip()\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    set_a, set_b = set(a), set(b)\n",
    "    union = set_a | set_b\n",
    "    inter = set_a & set_b\n",
    "    return len(inter) / len(union) if union else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "81be45bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/12] 联邦制药的UBT37034在超重/肥胖适应症方面取得了哪些临床前数据？...\n",
      "[2/12] 根据华创证券对凌云股份的深度研究报告，请问该公司在2024年的主要产品收入占比是多少？...\n",
      "[3/12] 关于凌云股份（600480）的热冲压技术应用和发展前景，能否详细解释热冲压成型工艺与冷冲压成型工艺的主要区别？...\n",
      "[4/12] 关于凌云股份（600480）的德国WAG业务板块及客户情况，请问具体有哪些主要客户？...\n",
      "[5/12] 广联达的数字施工业务在2020年的资金压力如何？与同行业其他企业相比，其资金压力有何特点？...\n",
      "[6/12] 如何评估广联达在数字化转型过程中面临的挑战及其应对策略？...\n",
      "[7/12] 如何评估广联达在数字化转型中的竞争优势？...\n",
      "[8/12] 如何分析广联达（002410.SZ）在2021年的PS估值水平及其与可比公司的差异？...\n",
      "[9/12] 千味央厨的餐饮大客户经营数据在2022年第三季度有何变化？...\n",
      "[10/12] 千味央厨公司在2020年的毛利率受原材料价格波动影响如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/12 [00:30<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/12] 关于伊利股份的历史发展和市场竞争，请问在2005年至2013年间，伊利如何通过创新产品和营销策略实现营收突破100亿大关...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/12 [00:35<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/12] 广联达（002410）的数字设计业务在2021年下半年将如何推进？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample: 100%|██████████| 12/12 [01:06<00:00,  5.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw predictions to: d:\\Datawhale\\Multimodal-RAG-Competitions\\eval_train_raw.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) Inference\n",
    "def run_one(idx):\n",
    "    item = train_data[idx]\n",
    "    q = item.get(\"question\", \"\")\n",
    "    tqdm.write(f\"[{sample_idx.index(idx)+1}/{len(sample_idx)}] {q[:60]}...\")\n",
    "    pred = rag.generate_answer(q, top_k=5)\n",
    "    return idx, pred\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as ex:\n",
    "    for out in tqdm(ex.map(run_one, sample_idx), total=len(sample_idx), desc=\"Infer on train sample\"):\n",
    "        results.append(out)\n",
    "\n",
    "# Save raw (idx, pred) for debugging\n",
    "with open(EVAL_RAW_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved raw predictions to: {EVAL_RAW_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26f5f2",
   "metadata": {},
   "source": [
    "## 6) Scoring vs Ground Truth\n",
    "Score per item:\n",
    "- page_match: 1 if exact page equals, else 0 (×0.25)\n",
    "- filename_match: 1 if exact filename equals, else 0 (×0.25)\n",
    "- answer_jaccard: char Jaccard (×0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1de5ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scored results to: d:\\Datawhale\\Multimodal-RAG-Competitions\\eval_train_scored.json\n",
      "max score: 0.5489690721649485\n",
      "Mean score: 0.2631\n",
      "min score: 0.08582089552238806\n",
      "Mean Jaccard: 0.3178\n",
      "Filename exact@1: 0.4167\n",
      "Page exact@1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 6) Score predictions\n",
    "idx2pred = {idx: pred for idx, pred in results}\n",
    "\n",
    "scored_rows = []\n",
    "for idx in sample_idx:\n",
    "    gt = train_data[idx]\n",
    "    pred = idx2pred.get(idx, {})\n",
    "\n",
    "    gt_q = gt.get(\"question\", \"\")\n",
    "    gt_a = gt.get(\"answer\", \"\")\n",
    "    gt_f = gt.get(\"filename\", \"\")\n",
    "    gt_p = gt.get(\"page\", \"\")\n",
    "\n",
    "    pr_a = pred.get(\"answer\", \"\")\n",
    "    pr_f = pred.get(\"filename\", \"\")\n",
    "    pr_p = pred.get(\"page\", \"\")\n",
    "\n",
    "    page_match = 1.0 if str(pr_p) == str(gt_p) else 0.0\n",
    "    filename_match = 1.0 if str(pr_f) == str(gt_f) else 0.0\n",
    "    answer_sim = jaccard_char(str(pr_a), str(gt_a))\n",
    "\n",
    "    score = 0.25 * page_match + 0.25 * filename_match + 0.5 * answer_sim\n",
    "\n",
    "    scored_rows.append({\n",
    "        \"idx\": idx,\n",
    "        \"question\": gt_q,\n",
    "        \"gt_answer\": gt_a,\n",
    "        \"pr_answer\": pr_a,\n",
    "        \"gt_filename\": gt_f,\n",
    "        \"pr_filename\": pr_f,\n",
    "        \"gt_page\": gt_p,\n",
    "        \"pr_page\": pr_p,\n",
    "        \"page_match\": page_match,\n",
    "        \"filename_match\": filename_match,\n",
    "        \"answer_jaccard\": answer_sim,\n",
    "        \"score\": score,\n",
    "    })\n",
    "\n",
    "# Sort by score ascending to inspect weak cases first\n",
    "scored_rows_sorted = sorted(scored_rows, key=lambda x: x[\"score\"])\n",
    "\n",
    "with open(EVAL_SUMMARY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scored_rows_sorted, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved scored results to: {EVAL_SUMMARY_PATH}\")\n",
    "print(f\"max score: {max(r['score'] for r in scored_rows_sorted)}\")\n",
    "print(f\"Mean score: {sum(r['score'] for r in scored_rows_sorted)/len(scored_rows_sorted):.4f}\")\n",
    "print(f\"min score: {min(r['score'] for r in scored_rows_sorted) }\")\n",
    "print(f\"Mean Jaccard: {sum(r['answer_jaccard'] for r in scored_rows_sorted)/len(scored_rows_sorted):.4f}\")\n",
    "print(f\"Filename exact@1: {sum(r['filename_match'] for r in scored_rows_sorted)/len(scored_rows_sorted):.4f}\")\n",
    "print(f\"Page exact@1: {sum(r['page_match'] for r in scored_rows_sorted)/len(scored_rows_sorted):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e7fda451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Worst cases —\n",
      "\n",
      "Score: 0.08582089552238806\n",
      "Q: 广联达的数字施工业务在2020年的资金压力如何？与同行业其他企业相比，其资金压力有何特点？\n",
      "GT: 根据图片中的图表和文字内容，可以得出以下结论：\n",
      "\n",
      "1. **资金压力情况**：\n",
      "   - 图表35显示，龙元建设在2015年至2020年间，应收账款占比和已完工未结算资产占比均呈现上升趋势。特别是在2015年到2016年间，应收账款占比和已完工未结算资产占比都有显著增加。\n",
      "\n",
      "2. **与其他企业的比较**：\n",
      "   - 图表36显示，宏润建设在2017年至2020年间，应收账款占比和建造合同形成的已完工未结算资产占比也\n",
      "PR: 2020年广联达施工业务资金压力主要源于智慧工地和解决方案类业务占比提升导致毛利率下降，但凭借造价市场的客户基础和SaaS化转型，其资金压力较同行业企业更可控。\n",
      "GT file/page: 广联达-再谈广联达当前时点下如何看待其三条增长曲线-220217131页.pdf 28\n",
      "PR file/page: 广联达-深度跟踪报告设计助推数字建筑一体化落地-22031838页.pdf 6\n",
      "\n",
      "— Best cases —\n",
      "\n",
      "Score: 0.5489690721649485\n",
      "Q: 联邦制药的UBT37034在超重/肥胖适应症方面取得了哪些临床前数据？\n",
      "GT: 根据图片中的文字内容，联邦制药的UBT37034在超重/肥胖适应症方面的临床前数据如下：\n",
      "\n",
      "1. UBT37034在饮食诱导肥胖大鼠（DIO Rats）上的临床前数据表明，21天给药后，UBT37034联用替尔泊肽减重13.6%。\n",
      "\n",
      "2. 相比之下，Petrelintide联用替尔泊肽减重13.6%（-9.38%），Cagrilintide联用替尔泊肽减\n",
      "PR: UBT37034在饮食诱导肥胖大鼠（DIO Rats）上21天给药后，联用替尔泊肽减重13.6%，优于Petrelintide联用替尔泊肽（-9.38%）、Cagrilintide联用替尔泊肽（-10.89%）及替尔泊肽单药（-3.02%）效果。\n",
      "GT file/page: 联邦制药-港股公司研究报告-创新突破三靶点战略联姻诺和诺德-25071225页.pdf 11\n",
      "PR file/page: 联邦制药-港股公司研究报告-创新突破三靶点战略联姻诺和诺德-25071225页.pdf 10\n"
     ]
    }
   ],
   "source": [
    "# Show a couple of worst and best cases inline (adjust k as needed)\n",
    "k = 1\n",
    "print(\"— Worst cases —\")\n",
    "for r in scored_rows_sorted[:k]:\n",
    "    print(\"\\nScore:\", r[\"score\"])\n",
    "    print(\"Q:\", r[\"question\"])\n",
    "    print(\"GT:\", r[\"gt_answer\"])\n",
    "    print(\"PR:\", r[\"pr_answer\"])\n",
    "    print(\"GT file/page:\", r[\"gt_filename\"], r[\"gt_page\"])\n",
    "    print(\"PR file/page:\", r[\"pr_filename\"], r[\"pr_page\"])\n",
    "\n",
    "print(\"\\n— Best cases —\")\n",
    "for r in scored_rows_sorted[-k:]:\n",
    "    print(\"\\nScore:\", r[\"score\"])\n",
    "    print(\"Q:\", r[\"question\"])\n",
    "    print(\"GT:\", r[\"gt_answer\"])\n",
    "    print(\"PR:\", r[\"pr_answer\"])\n",
    "    print(\"GT file/page:\", r[\"gt_filename\"], r[\"gt_page\"])\n",
    "    print(\"PR file/page:\", r[\"pr_filename\"], r[\"pr_page\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb39aa2",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Set a different **sample fraction** by changing the `0.10` in `math.ceil(N * 0.10)`.\n",
    "- If `filename`/`page` in ground truth differ in minor formatting (e.g., case, spaces), add normalization before comparison.\n",
    "- You can plug this same scorer later for validation on a dev split.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
