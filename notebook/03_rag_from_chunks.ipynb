{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1b152f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import hashlib\n",
    "from typing import List, Dict, Any\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import concurrent.futures\n",
    "import random\n",
    "\n",
    "from get_text_embedding import get_text_embedding\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e1ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import jieba, re\n",
    "\n",
    "def _preprocess(doc: str) -> str:\n",
    "    # 切分 6-8 位数字（日期/代码），弱化连字符/下划线\n",
    "    doc = re.sub(r'(\\d{6,8})', r' \\1 ', doc)\n",
    "    doc = doc.replace('-', ' ').replace('_', ' ')\n",
    "    return doc\n",
    "\n",
    "class FileBM25Index:\n",
    "    \"\"\"\n",
    "    文件级 BM25：用 文件名 + 前3页文本 + 标题/目录 行 来增强“版本可分性”\n",
    "    \"\"\"\n",
    "    def __init__(self, pages_per_file: int = 3, per_page_chars: int = 1500,\n",
    "                 repeat_filename_tokens: int = 2):\n",
    "        self.pages_per_file = pages_per_file\n",
    "        self.per_page_chars = per_page_chars\n",
    "        self.repeat_filename_tokens = repeat_filename_tokens\n",
    "        self.files = []\n",
    "        self.tokens = []\n",
    "        self.fn_to_idx = {}\n",
    "        self.bm25 = None\n",
    "\n",
    "    def build(self, chunks):\n",
    "        # 收集每文件的若干最早页 & 抓取标题行（MinerU markdown 常有 #/##/表/图）\n",
    "        pages_by_file = {}\n",
    "        titles_by_file = {}\n",
    "        for c in chunks:\n",
    "            fn = c[\"metadata\"][\"file_name\"]\n",
    "            pg = int(c[\"metadata\"][\"page\"])\n",
    "            txt = c[\"content\"]\n",
    "            # 标题/目录/图表行\n",
    "            titles = \"\\n\".join(\n",
    "                ln for ln in txt.splitlines()\n",
    "                if ln.lstrip().startswith(('#','##','表','图'))\n",
    "            )\n",
    "            if titles:\n",
    "                titles_by_file.setdefault(fn, []).append(titles[:500])\n",
    "\n",
    "            pages_by_file.setdefault(fn, {})\n",
    "            # 只收每页一次\n",
    "            if pg not in pages_by_file[fn]:\n",
    "                pages_by_file[fn][pg] = txt\n",
    "\n",
    "        self.files, self.tokens, self.fn_to_idx = [], [], {}\n",
    "        for fn, pgmap in pages_by_file.items():\n",
    "            first_pages = [pgmap[p] for p in sorted(pgmap)[: self.pages_per_file]]\n",
    "            titles = \"\\n\".join(titles_by_file.get(fn, []))[:1500]\n",
    "            # 文档：文件名 + 标题/目录摘要 + 前3页各1500字\n",
    "            doc = f\"{fn}\\n{titles}\\n\" + \"\\n\".join(fp[: self.per_page_chars] for fp in first_pages)\n",
    "            doc = _preprocess(doc)\n",
    "            toks = list(jieba.cut(doc))\n",
    "            # 额外重复文件名分词，放大“版本词/日期码”权重\n",
    "            toks += list(jieba.cut(_preprocess(fn))) * self.repeat_filename_tokens\n",
    "\n",
    "            self.fn_to_idx[fn] = len(self.files)\n",
    "            self.files.append(fn)\n",
    "            self.tokens.append(toks)\n",
    "\n",
    "        self.bm25 = BM25Okapi(self.tokens)\n",
    "\n",
    "    def top_files(self, query, n=25):\n",
    "        if not self.bm25:\n",
    "            return []\n",
    "        q = list(jieba.cut(_preprocess(query)))\n",
    "        scores = self.bm25.get_scores(q)\n",
    "        order = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "        return [(self.files[i], float(scores[i])) for i in order[:n]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6002c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rrf(rank, k=90):\n",
    "    \"\"\"\n",
    "    Reciprocal Rank Fusion 基本项。\n",
    "    约定 rank 为 0-based，这里内部转为 1-based 以避免极端放大。\n",
    "    k 越大越“温和”，推荐 90。\n",
    "    \"\"\"\n",
    "    r = int(rank) + 1\n",
    "    return 1.0 / (k + r)\n",
    "\n",
    "\n",
    "THEME_TOKENS = [\"二次创业\",\"三条增长曲线\",\"深度复盘\",\"稳定成长\",\"公司深度报告\",\"再谈\",\"走在修复的道路上\"]\n",
    "def extract_years(q): return set(re.findall(r\"20\\d{2}\", q))\n",
    "\n",
    "def soft_bias(ranked, question, year_boost=0.15, theme_boost=0.05):\n",
    "    years = extract_years(question)\n",
    "    themed = any(t in question for t in THEME_TOKENS)\n",
    "    for c in ranked:\n",
    "        fn = c[\"metadata\"][\"file_name\"]\n",
    "        s  = c.get(\"rerank_score\", 0.0)\n",
    "        if years and any(y in fn for y in years): s += year_boost\n",
    "        if themed: s += theme_boost if any(t in fn for t in THEME_TOKENS) else 0.0\n",
    "        c[\"rerank_score\"] = s\n",
    "    ranked.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "    return ranked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18779430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def file_vote(ranked_chunks, top_n_files=1, agg=\"sum\"):\n",
    "    buckets = defaultdict(list)\n",
    "    for c in ranked_chunks:\n",
    "        fn = c[\"metadata\"][\"file_name\"]\n",
    "        s  = c.get(\"rerank_score\", c.get(\"ret_score\", 0.0))\n",
    "        buckets[fn].append(float(s))\n",
    "\n",
    "    def agg_fn(v):\n",
    "        v = np.asarray(v, float)\n",
    "        return v.sum() if agg==\"sum\" else (v.mean() if agg==\"mean\" else v.max())\n",
    "\n",
    "    scored = sorted([(fn, agg_fn(v)) for fn, v in buckets.items()], key=lambda x: x[1], reverse=True)\n",
    "    keep = set(fn for fn,_ in scored[:top_n_files])\n",
    "    kept = [c for c in ranked_chunks if c[\"metadata\"][\"file_name\"] in keep]\n",
    "    return kept, (scored[0][0] if scored else None, dict(scored))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e717a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build windows from existing MinerU chunks: all_pdf_page_chunks.json -> all_pdf_windows.json RUN ONCE FIX page\n",
    "# import json, re, hashlib\n",
    "# from pathlib import Path\n",
    "\n",
    "# SRC = Path(\"./all_pdf_page_chunks_mineru.json\")          # MinerU output you already have\n",
    "# DST = Path(\"./all_pdf_windows_mineru.json\")              # new, windowized chunks\n",
    "\n",
    "# HDR = re.compile(r\"^(#{1,3})\\s+.+\", re.M)         # #, ##, ### headings\n",
    "# FIG = re.compile(r\"!\\[[^\\]]*\\]\\([^)]+\\)\")         # markdown images\n",
    "# TB  = re.compile(r\"<table[\\s\\S]*?</table>\", re.I)  # html tables\n",
    "\n",
    "# def _norm(s): return re.sub(r\"\\s+\", \" \", (s or \"\").strip())\n",
    "\n",
    "# def split_by_headers(md: str):\n",
    "#     # keep figure/table blocks as atomic spans\n",
    "#     blocks = []\n",
    "#     # placeholder tokens for figures/tables\n",
    "#     fig_tokens, tab_tokens = [], []\n",
    "#     def _stash(pattern, text, token_prefix):\n",
    "#         out, toks, i = text, [], 0\n",
    "#         for m in pattern.finditer(text):\n",
    "#             tok = f\"__{token_prefix}_{i}__\"\n",
    "#             toks.append((tok, m.group(0)))\n",
    "#             out = out.replace(m.group(0), tok, 1)\n",
    "#             i += 1\n",
    "#         return out, toks\n",
    "#     text, figs = _stash(FIG, md, \"FIG\")\n",
    "#     text, tabs = _stash(TB,  text, \"TAB\")\n",
    "\n",
    "#     # split by headers; if none, single block\n",
    "#     parts = []\n",
    "#     last = 0\n",
    "#     for m in HDR.finditer(text):\n",
    "#         if m.start() > last:\n",
    "#             parts.append(text[last:m.start()])\n",
    "#         parts.append(text[m.start():m.end()])  # header line as its own piece\n",
    "#         last = m.end()\n",
    "#     if last < len(text):\n",
    "#         parts.append(text[last:])\n",
    "\n",
    "#     # restore tokens\n",
    "#     def _restore(s, toks):\n",
    "#         for tok, val in toks:\n",
    "#             s = s.replace(tok, val)\n",
    "#         return s\n",
    "#     parts = [_restore(_restore(p, figs), tabs) for p in parts]\n",
    "#     # drop empties\n",
    "#     parts = [p for p in parts if _norm(p)]\n",
    "#     return parts\n",
    "\n",
    "# def windowize(parts, target=900, overlap=150):\n",
    "#     out, buf = [], \"\"\n",
    "#     for p in parts:\n",
    "#         p = _norm(p)\n",
    "#         if len(p) <= target:\n",
    "#             if len(buf) + len(p) + 1 <= target:\n",
    "#                 buf = (buf + \" \" + p).strip()\n",
    "#             else:\n",
    "#                 if buf: out.append(buf)\n",
    "#                 tail = buf[-overlap:] if buf and overlap else \"\"\n",
    "#                 buf = (tail + \" \" + p).strip()\n",
    "#         else:\n",
    "#             # long piece -> sliding windows\n",
    "#             stride = max(50, target - overlap)\n",
    "#             i = 0\n",
    "#             while i < len(p):\n",
    "#                 out.append(_norm(p[i:i+target]))\n",
    "#                 i += stride\n",
    "#     if buf: out.append(buf)\n",
    "#     return out\n",
    "\n",
    "# print(\"Loading MinerU page chunks…\")\n",
    "# chunks = json.loads(Path(SRC).read_text(encoding=\"utf-8\"))\n",
    "# print(f\"Loaded {len(chunks)} page-chunks\")\n",
    "\n",
    "# seen, windows = set(), []\n",
    "# for c in chunks:\n",
    "#     file = c[\"metadata\"][\"file_name\"]\n",
    "#     page = int(c[\"metadata\"][\"page\"])+1   # shift 1 , quick fix for the fucked up in mineru process\n",
    "#     parts = split_by_headers(c[\"content\"])\n",
    "#     wins  = windowize(parts, target=900, overlap=150)\n",
    "#     for widx, w in enumerate(wins):\n",
    "#         h = hashlib.md5(_norm(w).encode()).hexdigest()\n",
    "#         if h in seen: \n",
    "#             continue\n",
    "#         seen.add(h)\n",
    "#         windows.append({\n",
    "#             \"id\": f\"{c['id']}_w{widx}\",\n",
    "#             \"content\": w,\n",
    "#             \"metadata\": {\n",
    "#                 \"file_name\": file,\n",
    "#                 \"page\": page,         \n",
    "#                 \"widx\": widx\n",
    "#             }\n",
    "#         })\n",
    "\n",
    "# print(f\"Built {len(windows)} windows\")\n",
    "# DST.write_text(json.dumps(windows, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "# print(f\"Saved -> {DST}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1597c84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "#   Score drop IDK\n",
    "# def file_vote(ranked_chunks, top_n_files=1, agg=\"sum\"):\n",
    "#     \"\"\"\n",
    "#     Aggregate scores per filename and keep chunks from the winning file(s).\n",
    "#     ranked_chunks: reranked list (desc), each with [\"metadata\"][\"file_name\"] and \"rerank_score\".\n",
    "#     \"\"\"\n",
    "#     buckets = defaultdict(list)\n",
    "#     for c in ranked_chunks:\n",
    "#         fn = c[\"metadata\"][\"file_name\"]\n",
    "#         s = c.get(\"rerank_score\", c.get(\"ret_score\", 0.0))\n",
    "#         buckets[fn].append(float(s))\n",
    "\n",
    "#     def agg_fn(v):\n",
    "#         v = np.asarray(v, float)\n",
    "#         if agg == \"mean\": return v.mean()\n",
    "#         if agg == \"max\":  return v.max()\n",
    "#         return v.sum()  # default\n",
    "\n",
    "#     scored = sorted([(fn, agg_fn(v)) for fn, v in buckets.items()],\n",
    "#                     key=lambda x: x[1], reverse=True)\n",
    "#     keep_files = set(fn for fn, _ in scored[:top_n_files])\n",
    "#     kept = [c for c in ranked_chunks if c[\"metadata\"][\"file_name\"] in keep_files]\n",
    "#     # Keep a tiny meta for telemetry\n",
    "#     return kept, (scored[0][0] if scored else None, dict(scored))\n",
    "\n",
    "def page_vote(ranked_chunks, top_m_pages=1, agg=\"max\"):\n",
    "    \"\"\"\n",
    "    ranked_chunks: list already reranked by your reranker (desc)\n",
    "    Aggregate scores per (file_name, page) and return chunks from the winning page(s).\n",
    "    \"\"\"\n",
    "    buckets = defaultdict(list)\n",
    "    for c in ranked_chunks:\n",
    "        key = (c[\"metadata\"][\"file_name\"], int(c[\"metadata\"][\"page\"]))\n",
    "        s = c.get(\"rerank_score\", c.get(\"ret_score\", 0.0))\n",
    "        buckets[key].append(float(s))\n",
    "\n",
    "    def agg_fn(v):\n",
    "        v = np.asarray(v, float)\n",
    "        if agg == \"sum\": return v.sum()\n",
    "        if agg == \"mean\": return v.mean()\n",
    "        return v.max()\n",
    "\n",
    "    scored_pages = sorted([(k, agg_fn(v)) for k, v in buckets.items()], key=lambda x: x[1], reverse=True)\n",
    "    keep_keys = set(k for k,_ in scored_pages[:top_m_pages])\n",
    "    return [c for c in ranked_chunks if (c[\"metadata\"][\"file_name\"], int(c[\"metadata\"][\"page\"])) in keep_keys]\n",
    "\n",
    "def expand_neighbors_on_page(page_chunks, radius=1, max_total=8):\n",
    "    # Assumes all chunks are from the same (file,page); use their widx to add neighbors\n",
    "    page_chunks = sorted(page_chunks, key=lambda x: x[\"metadata\"].get(\"widx\", 0))\n",
    "    out = []\n",
    "    picked = set()\n",
    "    for c in page_chunks[:max_total]:\n",
    "        w = c[\"metadata\"].get(\"widx\", 0)\n",
    "        for j in range(max(0, w - radius), min(len(page_chunks), w + radius + 1)):\n",
    "            cj = page_chunks[j]\n",
    "            key = cj[\"metadata\"].get(\"widx\", j)\n",
    "            if key in picked: \n",
    "                continue\n",
    "            picked.add(key); out.append(cj)\n",
    "            if len(out) >= max_total: \n",
    "                return out\n",
    "    return out or page_chunks[:max_total]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8697dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, time, requests\n",
    "\n",
    "class SiliconFlowReranker:\n",
    "    def __init__(self, api_key: str = None,\n",
    "                 model: str = \"BAAI/bge-reranker-v2-m3\",\n",
    "                 endpoint: str = \"https://api.siliconflow.cn/v1/rerank\",\n",
    "                 timeout: int = 30,\n",
    "                 max_retries: int = 4,\n",
    "                 add_header: bool = True,\n",
    "                 header_snippet_chars: int = 300):\n",
    "        self.api_key = api_key or os.getenv('LOCAL_API_KEY')\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Missing SILICONFLOW_API_KEY\")\n",
    "        self.model = model\n",
    "        self.endpoint = endpoint\n",
    "        self.timeout = timeout\n",
    "        self.max_retries = max_retries\n",
    "\n",
    "        # NEW\n",
    "        self.add_header = add_header\n",
    "        self.header_snippet_chars = header_snippet_chars\n",
    "\n",
    "    def _extract_date_from_filename(self, fn: str) -> str:\n",
    "        \"\"\"\n",
    "        Try to parse YYYY[-_.]?MM[-_.]?DD in filename and normalize to YYYY-MM-DD.\n",
    "        Returns '' if not found.\n",
    "        \"\"\"\n",
    "        m = re.search(r'(20\\d{2})[-_.]?(0[1-9]|1[0-2])[-_.]?(0[1-9]|[12]\\d|3[01])', fn)\n",
    "        if not m: return ''\n",
    "        y, mo, d = m.groups()\n",
    "        return f\"{y}-{mo}-{d}\"\n",
    "\n",
    "    def _make_header(self, c: dict) -> str:\n",
    "        meta = c.get(\"metadata\", {})\n",
    "        fn = str(meta.get(\"file_name\", \"\"))\n",
    "        pg = str(meta.get(\"page\", \"\"))\n",
    "        dt = self._extract_date_from_filename(fn)\n",
    "        snippet = c.get(\"content\", \"\")[: self.header_snippet_chars]\n",
    "        parts = [f\"【文件】{fn}\", f\"【页】{pg}\"]\n",
    "        if dt: parts.append(f\"【日期】{dt}\")\n",
    "        parts.append(f\"【摘要】{snippet}\")\n",
    "        return \"\\n\".join(parts) + \"\\n\"\n",
    "\n",
    "    def rerank(self, question: str, candidates: list, return_meta: bool = False):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          ranked_chunks, meta where meta = {\"status\": http_status or None, \"attempts\": N}\n",
    "        Retries on 429/5xx with exponential backoff.\n",
    "        \"\"\"\n",
    "        # Build documents (with optional header)\n",
    "        if self.add_header:\n",
    "            documents = [self._make_header(c) + c[\"content\"] for c in candidates]\n",
    "        else:\n",
    "            documents = [c[\"content\"] for c in candidates]\n",
    "\n",
    "        payload = {\"model\": self.model, \"query\": question, \"documents\": documents}\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "        status = None\n",
    "        attempts = 0\n",
    "        data = None\n",
    "\n",
    "        for attempt in range(self.max_retries):\n",
    "            attempts = attempt + 1\n",
    "            try:\n",
    "                r = requests.post(self.endpoint, json=payload, headers=headers, timeout=self.timeout)\n",
    "                status = r.status_code\n",
    "                if status in (429, 500, 502, 503, 504):\n",
    "                    time.sleep(2 ** attempt)  # backoff\n",
    "                    continue\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "                break\n",
    "            except requests.RequestException:\n",
    "                time.sleep(2 ** attempt)\n",
    "\n",
    "        ranked = []\n",
    "        if data:\n",
    "            results = data.get(\"results\") or data.get(\"data\") or []\n",
    "            seen = set()\n",
    "            for res in results:\n",
    "                idx = res.get(\"index\")\n",
    "                if idx is None or idx >= len(candidates):\n",
    "                    continue\n",
    "                seen.add(idx)\n",
    "                c = dict(candidates[idx])\n",
    "                c[\"rerank_score\"] = float(res.get(\"relevance_score\", 0.0))\n",
    "                ranked.append(c)\n",
    "            # fill any missing\n",
    "            for i, c in enumerate(candidates):\n",
    "                if i not in seen:\n",
    "                    cc = dict(c); cc[\"rerank_score\"] = 0.0\n",
    "                    ranked.append(cc)\n",
    "            ranked.sort(key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "\n",
    "        meta = {\"status\": status, \"attempts\": attempts}\n",
    "        return (ranked, meta) if return_meta else ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add01a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageChunkLoader:\n",
    "    def __init__(self, json_path: str):\n",
    "        self.json_path = json_path\n",
    "    def load_chunks(self) -> List[Dict[str, Any]]:\n",
    "        with open(self.json_path, 'r', encoding='utf-8') as f:\n",
    "            return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbb00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel:\n",
    "    def __init__(self, batch_size: int = 64):\n",
    "        self.api_key = os.getenv('LOCAL_API_KEY')\n",
    "        self.base_url = os.getenv('LOCAL_BASE_URL')\n",
    "        self.embedding_model = os.getenv('LOCAL_EMBEDDING_MODEL')\n",
    "        self.batch_size = batch_size\n",
    "        if not self.api_key or not self.base_url:\n",
    "            raise ValueError('请在.env中配置LOCAL_API_KEY和LOCAL_BASE_URL')\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> List[List[float]]:\n",
    "        return get_text_embedding(\n",
    "            texts,\n",
    "            api_key=self.api_key,\n",
    "            base_url=self.base_url,\n",
    "            embedding_model=self.embedding_model,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "    def embed_text(self, text: str) -> List[float]:\n",
    "        return self.embed_texts([text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e741cd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    def __init__(self):\n",
    "        self.embeddings = []\n",
    "        self.chunks = []\n",
    "    def add_chunks(self, chunks: List[Dict[str, Any]], embeddings: List[List[float]]):\n",
    "        self.chunks.extend(chunks)\n",
    "        self.embeddings.extend(embeddings)\n",
    "    def search(self, query_embedding: List[float], top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        from numpy import dot\n",
    "        from numpy.linalg import norm\n",
    "        import numpy as np\n",
    "        if not self.embeddings:\n",
    "            return []\n",
    "        emb_matrix = np.array(self.embeddings)\n",
    "        query_emb = np.array(query_embedding)\n",
    "        sims = emb_matrix @ query_emb / (norm(emb_matrix, axis=1) * norm(query_emb) + 1e-8)\n",
    "        idxs = sims.argsort()[::-1][:top_k]\n",
    "        return [self.chunks[i] for i in idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e391bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def pick_best_page_and_expand(file_scoped, final_k=5, radius=1):\n",
    "    # group by page\n",
    "    by_page = defaultdict(list)\n",
    "    for c in file_scoped:\n",
    "        by_page[int(c[\"metadata\"][\"page\"])].append(c)\n",
    "    # score each page by max rerank_score\n",
    "    page_scores = [(pg, max(float(x.get(\"rerank_score\", 0.0)) for x in cs))\n",
    "                   for pg, cs in by_page.items()]\n",
    "    best_pg, _ = max(page_scores, key=lambda t: t[1])\n",
    "\n",
    "    page_chunks = sorted(by_page[best_pg], key=lambda x: x[\"metadata\"].get(\"widx\", 0))\n",
    "    # expand around the highest-scored chunk on that page\n",
    "    top_idx = max(range(len(page_chunks)),\n",
    "                  key=lambda i: float(page_chunks[i].get(\"rerank_score\", 0.0)))\n",
    "    w0 = page_chunks[top_idx][\"metadata\"].get(\"widx\", top_idx)\n",
    "\n",
    "    # collect neighbors by widx\n",
    "    wanted = {w0}\n",
    "    for r in range(1, radius+1):\n",
    "        wanted.add(w0 - r); wanted.add(w0 + r)\n",
    "\n",
    "    chosen = [c for c in page_chunks if c[\"metadata\"].get(\"widx\", -10**9) in wanted]\n",
    "    # backfill with other chunks on same page if needed\n",
    "    if len(chosen) < final_k:\n",
    "        for c in page_chunks:\n",
    "            if c not in chosen:\n",
    "                chosen.append(c)\n",
    "            if len(chosen) >= final_k: break\n",
    "    return chosen[:final_k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b79cfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List, Optional\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "class SimpleRAG:\n",
    "    def __init__(\n",
    "        self,\n",
    "        chunk_json_path: str,\n",
    "        model_path: str = None,\n",
    "        batch_size: int = 32,\n",
    "        use_rerank: bool = False,\n",
    "        candidate_k: int = 120,\n",
    "        final_k: int = 5,\n",
    "        reranker=None,\n",
    "        \n",
    "    ):\n",
    "        self.loader = PageChunkLoader(chunk_json_path)\n",
    "        self.embedding_model = EmbeddingModel(batch_size=batch_size)\n",
    "        self.vector_store = SimpleVectorStore()\n",
    "        \n",
    "\n",
    "        # Rerank controls\n",
    "        self.use_rerank = use_rerank\n",
    "        self.candidate_k = candidate_k\n",
    "        self.final_k = final_k\n",
    "        self.reranker = reranker\n",
    "        \n",
    "\n",
    "        # Behavior flags\n",
    "        self.lock_source     = bool(int(os.getenv(\"LOCK_SOURCE\",  \"0\")))\n",
    "        self.strict_rerank   = bool(int(os.getenv(\"STRICT_RERANK\",\"0\")))\n",
    "        self.debug_telemetry = bool(int(os.getenv(\"DEBUG_RAG\",    \"1\")))\n",
    "\n",
    "        self.limit_per_file = int(os.getenv(\"LIMIT_PER_FILE\", \"12\"))\n",
    "\n",
    "\n",
    "        # # Retrieval helpers\n",
    "        # self.file_bm25 = FileBM25Index(pages_per_file=3, per_page_chars=1500, repeat_filename_tokens=2)\n",
    "        # self.file_bm25.build(self.chunks)\n",
    "\n",
    "\n",
    "        # Hold corpus if needed elsewhere\n",
    "        self.chunks = None                 # <-- set in setup()\n",
    "\n",
    "    def setup(self):\n",
    "        print(\"加载所有页chunk...\")\n",
    "        self.chunks = self.loader.load_chunks()\n",
    "        print(f\"共加载 {len(self.chunks)} 个chunk\")\n",
    "\n",
    "        print(\"生成嵌入...\")\n",
    "        embeddings = self.embedding_model.embed_texts([c[\"content\"] for c in self.chunks])\n",
    "\n",
    "        print(\"存储向量...\")\n",
    "        self.vector_store.add_chunks(self.chunks, embeddings)\n",
    "\n",
    "        # Build file-level BM25 now that chunks exist\n",
    "        print(\"构建文件级BM25索引...\")\n",
    "        self.file_bm25 = FileBM25Index(pages_per_file=3, per_page_chars=1500, repeat_filename_tokens=2)\n",
    "        self.file_bm25.build(self.chunks)\n",
    "\n",
    "        print(\"RAG向量库构建完成！\")\n",
    "\n",
    "    def query(self, question: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "        q_emb = self.embedding_model.embed_text(question)\n",
    "        results = self.vector_store.search(q_emb, top_k)\n",
    "        return {\"question\": question, \"chunks\": results}\n",
    "\n",
    "    def _build_context(self, items: List[Dict[str, Any]]) -> str:\n",
    "        return \"\\n\".join(\n",
    "            f\"[文件名]{c['metadata']['file_name']} [页码]{c['metadata']['page']}\\n{c['content']}\"\n",
    "            for c in items\n",
    "        )\n",
    "\n",
    "    def generate_answer(self, question: str, top_k: int = 3) -> Dict[str, Any]:\n",
    "        qwen_api_key = os.getenv(\"LOCAL_API_KEY\")\n",
    "        qwen_base_url = os.getenv(\"LOCAL_BASE_URL\")\n",
    "        qwen_model    = os.getenv(\"LOCAL_TEXT_MODEL\")\n",
    "        if not qwen_api_key or not qwen_base_url or not qwen_model:\n",
    "            raise ValueError(\"请在.env中配置LOCAL_API_KEY、LOCAL_BASE_URL、LOCAL_TEXT_MODEL\")\n",
    "\n",
    "        tele = {\"path\":\"\", \"rerank_ok\":False, \"rerank_attempts\":0, \"rerank_http\":None,\n",
    "                \"cand_n\":0, \"ranked_n\":0, \"file_vote_best\":None, \"file_vote_n\":0, \"final_n\":0}\n",
    "\n",
    "        # Stage-A dense\n",
    "        q_emb = self.embedding_model.embed_text(question)\n",
    "        candidates = self.vector_store.search(q_emb, self.candidate_k)\n",
    "        tele[\"cand_n\"] = len(candidates)\n",
    "\n",
    "        # Optional per-file diversity cap\n",
    "        if candidates and self.limit_per_file > 0:\n",
    "            by_file, diverse = {}, []\n",
    "            for c in candidates:\n",
    "                fn = c[\"metadata\"][\"file_name\"]\n",
    "                by_file.setdefault(fn, 0)\n",
    "                if by_file[fn] < self.limit_per_file:\n",
    "                    diverse.append(c); by_file[fn] += 1\n",
    "            candidates = diverse\n",
    "\n",
    "        # RRF fuse with file-level BM25 (only if built)\n",
    "        if self.file_bm25 and candidates:\n",
    "            # 记录 BEFORE (先存，再融合)\n",
    "            pre_rrf_files = list(dict.fromkeys([c[\"metadata\"][\"file_name\"] for c in candidates]))\n",
    "            tele[\"first3_files_before_rrf\"] = pre_rrf_files[:3]\n",
    "\n",
    "            # 参数\n",
    "            BM25_TOPN = int(os.getenv(\"BM25_TOPN\", \"25\"))\n",
    "            RRF_K     = int(os.getenv(\"RRF_K\", \"90\"))\n",
    "\n",
    "            # BM25 文件排名\n",
    "            top_files = self.file_bm25.top_files(question, n=BM25_TOPN)\n",
    "            tele[\"bm25_top_files\"] = [fn for fn, _ in top_files[:5]]\n",
    "\n",
    "            # 融合（统一用 RRF_K；rank 约定 0-based -> rrf 内部会 +1）\n",
    "            file_to_rank = {fn: r for r, (fn, _) in enumerate(top_files)}\n",
    "            BIG = 10_000\n",
    "\n",
    "            fused = []\n",
    "            for i, c in enumerate(candidates):\n",
    "                fn = c[\"metadata\"][\"file_name\"]\n",
    "                dense_rank = i\n",
    "                bm25_rank  = file_to_rank.get(fn, BIG)\n",
    "                score = rrf(dense_rank, k=RRF_K) + rrf(bm25_rank, k=RRF_K)\n",
    "                cc = dict(c); cc[\"fused_score\"] = score\n",
    "                fused.append(cc)\n",
    "\n",
    "            candidates = sorted(fused, key=lambda x: x[\"fused_score\"], reverse=True)\n",
    "\n",
    "        # 记录 AFTER（放在 if 外也行，candidates 一定存在）\n",
    "        tele[\"first3_files_after_rrf\"] = list(dict.fromkeys([c[\"metadata\"][\"file_name\"] for c in candidates]))[:3]\n",
    "\n",
    "\n",
    "        # Rerank → soft bias → file_vote → neighbor expansion\n",
    "        if self.use_rerank and self.reranker is not None and candidates:\n",
    "            tele[\"path\"] = \"rerank\"\n",
    "            ranked, rr_meta = self.reranker.rerank(question, candidates, return_meta=True)\n",
    "            tele[\"rerank_http\"] = rr_meta.get(\"status\"); tele[\"rerank_attempts\"] = rr_meta.get(\"attempts\")\n",
    "            tele[\"ranked_n\"] = len(ranked); tele[\"rerank_ok\"] = bool(ranked)\n",
    "\n",
    "            if not ranked:\n",
    "                if self.strict_rerank:\n",
    "                    raise RuntimeError(f\"Rerank failed (status={tele['rerank_http']}) and STRICT_RERANK=1\")\n",
    "                chunks = candidates[: self.final_k]\n",
    "            else:\n",
    "                ranked = soft_bias(ranked, question)  # cheap nudge on year/theme\n",
    "                file_scoped, (best_file, _) = file_vote(ranked, top_n_files=1, agg=\"max\") # Change sum to MAX\n",
    "                chunks = pick_best_page_and_expand(file_scoped, final_k=self.final_k, radius=1)\n",
    "\n",
    "                # no page_vote; just take neighbors within the chosen file\n",
    "                try:\n",
    "                    chunks = expand_neighbors_on_page(file_scoped, radius=1, max_total=self.final_k)\n",
    "                except Exception:\n",
    "                    chunks = file_scoped[: self.final_k]\n",
    "        else:\n",
    "            tele[\"path\"] = \"baseline\"\n",
    "            chunks = candidates[: top_k]\n",
    "\n",
    "        tele[\"final_n\"] = len(chunks)\n",
    "\n",
    "        # Early exit\n",
    "        if not chunks:\n",
    "            out = {\"question\": question, \"answer\": \"\", \"filename\": \"\", \"page\": \"\", \"retrieval_chunks\": []}\n",
    "            if self.debug_telemetry: out[\"debug\"] = tele\n",
    "            return out\n",
    "\n",
    "        # Evidence for LOCK_SOURCE\n",
    "        top_file = chunks[0]['metadata']['file_name']\n",
    "        top_page = chunks[0]['metadata']['page']\n",
    "\n",
    "        # Build context\n",
    "        context = self._build_context(chunks)\n",
    "\n",
    "        # LLM call\n",
    "        client = OpenAI(api_key=qwen_api_key, base_url=qwen_base_url)\n",
    "        prompt = (\n",
    "            \"你是一名专业的金融分析助手，请根据以下检索到的内容回答用户问题。\\n\"\n",
    "            \"请严格按照如下JSON格式输出：\\n\"\n",
    "            '{\"answer\": \"你的简洁回答\", \"filename\": \"来源文件名\", \"page\": \"来源页码\"}\\n'\n",
    "            f\"检索内容：\\n{context}\\n\\n问题：{question}\\n\"\n",
    "            \"请确保输出内容为合法JSON字符串，不要输出多余内容。\"\n",
    "        )\n",
    "        completion = client.chat.completions.create(\n",
    "            model=qwen_model,\n",
    "            messages=[{\"role\": \"system\", \"content\": \"你是一名专业的金融分析助手。\"},\n",
    "                      {\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2, max_tokens=1024,\n",
    "        )\n",
    "\n",
    "        import json as pyjson\n",
    "        from extract_json_array import extract_json_array\n",
    "        raw = completion.choices[0].message.content.strip()\n",
    "        json_str = extract_json_array(raw, mode=\"objects\")\n",
    "\n",
    "        if json_str:\n",
    "            try:\n",
    "                arr = pyjson.loads(json_str)\n",
    "                if isinstance(arr, list) and arr:\n",
    "                    j = arr[0]\n",
    "                    answer = j.get(\"answer\", \"\"); filename = j.get(\"filename\", \"\"); page = j.get(\"page\", \"\")\n",
    "                else:\n",
    "                    answer, filename, page = raw, top_file, top_page\n",
    "            except Exception:\n",
    "                answer, filename, page = raw, top_file, top_page\n",
    "        else:\n",
    "            answer, filename, page = raw, top_file, top_page\n",
    "\n",
    "        # normalize page & apply offset\n",
    "        PAGE_OFFSET = int(os.getenv(\"PAGE_OFFSET\", \"0\"))\n",
    "        try:\n",
    "            if page not in (\"\", None): page = int(page) + PAGE_OFFSET\n",
    "        except Exception:\n",
    "            try: page = int(top_page) + PAGE_OFFSET\n",
    "            except Exception: page = top_page\n",
    "\n",
    "        # LOCK_SOURCE\n",
    "        model_file, model_page = filename, page\n",
    "        if self.lock_source and chunks:\n",
    "            filename, page = top_file, (int(top_page) + PAGE_OFFSET if str(top_page).isdigit() else top_page)\n",
    "\n",
    "        out = {\"question\": question, \"answer\": answer, \"filename\": filename, \"page\": page, \"retrieval_chunks\": chunks}\n",
    "        if self.debug_telemetry:\n",
    "            tele.update({\"top_file\": top_file, \"top_page\": top_page, \"model_file\": model_file, \"model_page\": model_page})\n",
    "            out[\"debug\"] = tele\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04e17890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook Dir: d:\\Datawhale\\Multimodal-RAG-Competitions\\notebook\n",
      "Project Root : d:\\Datawhale\\Multimodal-RAG-Competitions\n",
      "Train JSON   : d:\\Datawhale\\Multimodal-RAG-Competitions\\data\\train.json\n",
      "Chunks JSON  : d:\\Datawhale\\Multimodal-RAG-Competitions\\notebook\\all_pdf_windows_mineru.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuch\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 1) Imports & Paths\n",
    "from pathlib import Path\n",
    "import os, json, random\n",
    "from tqdm.auto import tqdm\n",
    "import concurrent.futures\n",
    "import math\n",
    "\n",
    "# Notebook is in .../notebook; project root is parent\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJ_ROOT = NOTEBOOK_DIR.parent\n",
    "\n",
    "# Try common locations for train.json\n",
    "CANDIDATE_TRAIN = [\n",
    "    PROJ_ROOT / \"datas\" / \"train.json\",\n",
    "    PROJ_ROOT / \"data\" / \"train.json\",\n",
    "    NOTEBOOK_DIR / \"datas\" / \"train.json\",\n",
    "    NOTEBOOK_DIR / \"data\" / \"train.json\",\n",
    "]\n",
    "TRAIN_PATH = next((p for p in CANDIDATE_TRAIN if p.exists()), None)\n",
    "if TRAIN_PATH is None:\n",
    "    raise FileNotFoundError(f\"train.json not found in: {CANDIDATE_TRAIN}\")\n",
    "\n",
    "# Chunk JSON path (your earlier structure)\n",
    "CHUNK_JSON_PATH = PROJ_ROOT / \"notebook\" / \"all_pdf_windows_mineru.json\"\n",
    "\n",
    "# Outputs\n",
    "EVAL_RAW_PATH = PROJ_ROOT / \"eval_train_raw.json\"\n",
    "EVAL_SUMMARY_PATH = PROJ_ROOT / \"eval_train_scored.json\"\n",
    "\n",
    "print(\"Notebook Dir:\", NOTEBOOK_DIR)\n",
    "print(\"Project Root :\", PROJ_ROOT)\n",
    "print(\"Train JSON   :\", TRAIN_PATH)\n",
    "print(\"Chunks JSON  :\", CHUNK_JSON_PATH)\n",
    "\n",
    "# --- RAG behavior flags (your SimpleRAG reads these) ---\n",
    "env = os.environ.copy()\n",
    "env.update({\n",
    "    \"LOCK_SOURCE\":  \"1\",   # force filename/page to top evidence\n",
    "    \"STRICT_RERANK\":\"0\",   # set \"1\" to fail fast if rerank API hiccups\n",
    "    \"DEBUG_RAG\":    \"1\",   # include telemetry in outputs\n",
    "    \"LIMIT_PER_FILE\": \"12\",\n",
    "    \"PAGE_OFFSET\":  \"0\",   # MinerU 0-based -> leaderboard 1-based\n",
    "    \"BM25_TOPN\":\"25\",\n",
    "    \"RRF_K\":\"90\",\n",
    "})\n",
    "os.environ.update(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7e5722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载所有页chunk...\n",
      "共加载 10053 个chunk\n",
      "生成嵌入...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|██████████| 315/315 [05:53<00:00,  1.12s/batch]\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\liuch\\AppData\\Local\\Temp\\jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "存储向量...\n",
      "构建文件级BM25索引...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.713 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG向量库构建完成！\n"
     ]
    }
   ],
   "source": [
    "# Create your reranker beforehand\n",
    "# reranker = HybridReranker(model_name=\"BAAI/bge-reranker-base\")\n",
    "reranker = SiliconFlowReranker()\n",
    "rag = SimpleRAG(\n",
    "    chunk_json_path=CHUNK_JSON_PATH,  # or your page/block chunk file\n",
    "    use_rerank=True,\n",
    "    candidate_k=120,\n",
    "    final_k=5,\n",
    "    reranker=reranker,\n",
    ")\n",
    "rag.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7135184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size = 118 | Sample size = 118\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Load train and sample\n",
    "with open(TRAIN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "N = len(train_data)\n",
    "random.seed(42)\n",
    "\n",
    "sample_size = max(1, math.ceil(N * 1))\n",
    "all_idx = list(range(N))\n",
    "sample_idx = sorted(random.sample(all_idx, sample_size)) if sample_size < N else all_idx\n",
    "\n",
    "print(f\"Train size = {N} | Sample size = {len(sample_idx)}\")\n",
    "sample_idx[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e777c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Jaccard helper\n",
    "def jaccard_char(a: str, b: str) -> float:\n",
    "    a = (a or \"\").strip()\n",
    "    b = (b or \"\").strip()\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    set_a, set_b = set(a), set(b)\n",
    "    union = set_a | set_b\n",
    "    inter = set_a & set_b\n",
    "    return len(inter) / len(union) if union else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81be45bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/118] 根据联邦制药（03933.HK）的公司发展历程，请简述其在2023年的重大产品临床进展。...\n",
      "[2/118] 联邦制药的UBT251与诺和诺德达成的20亿美元BD协议，对UBT251的临床进展有何影响？...\n",
      "[3/118] 联邦制药的UBT251在减重适应症方面的研发进展如何？...\n",
      "[4/118] 联邦制药的UBT37034在超重/肥胖适应症方面取得了哪些临床前数据？...\n",
      "[5/118] 联邦制药的TUL01101片剂和软膏剂在特应性皮炎治疗中的疗效如何？...\n",
      "[6/118] 根据《联邦制药-港股公司研究报告-创新突破三靶点战略联姻诺和诺德-250712》文件内容，请问联邦制药在2024年的营业...\n",
      "[7/118] 联邦制药的动保业务在2024年的市场表现如何？...\n",
      "[8/118] 联邦制药（03933.HK）的动保业务未来发展前景如何？...\n",
      "[9/118] 联邦制药在2024年的胰岛素集采中，其三代胰岛素的市场份额有何变化？...\n",
      "[10/118] 极兔速递-W：根据图表分析，2023年东南亚各国电商GMV增速最快的国家是哪一个？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/118 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/118 [00:28<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11/118] 极兔速递-W，根据图表和文字信息，请分析东南亚电商市场的发展潜力及其对极兔速递的影响。...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/118 [00:29<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12/118] 极兔速递-W：2024年单位运输成本和分拣成本分别下降了多少百分比？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/118 [00:29<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13/118] 极兔速递-W的跨境物流业务未来发展方向如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/118 [00:31<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/118] 根据华创证券对凌云股份的深度研究报告，请问该公司在2024年的主要产品收入占比是多少？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/118 [00:31<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15/118] 关于凌云股份（600480）的热冲压技术应用和发展前景，能否详细解释热冲压成型工艺与冷冲压成型工艺的主要区别？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/118 [00:32<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16/118] 根据华创证券对凌云股份（600480）的深度研究报告，请问该公司在2024年的热成型客户拓展情况如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/118 [00:35<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/118] 关于凌云股份（600480）的电池盒业务，如何分析其在新能源汽车渗透率提升下的市场潜力？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   0%|          | 0/118 [00:41<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18/118] 关于凌云股份（600480）的德国WAG业务板块及客户情况，请问具体有哪些主要客户？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [00:42<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19/118] 关于凌云股份（600480）的热成型电池盒双轮驱动传感器加速布局，请问其在新能源汽车领域的具体布局情况如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [00:51<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20/118] 根据凌云股份（600480）的深度研究报告，请问亚大集团在乘用车和商用车管路产品应用方面分别有哪些具体的产品？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:02<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21/118] 关于凌云股份（600480）的力传感器业务，具体应用场景有哪些？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:05<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22/118] 如何分析广联达在2006年至2020年间营业收入和净利润的增长趋势？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:06<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23/118] 如何分析广联达在“八三”规划中设立的“3+X”业务发展目标？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:07<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24/118] 广联达的股权激励措施对公司的人效和薪酬有何影响？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:11<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25/118] 广联达：根据2016至2020年中国房建领域房屋竣工价值结构（按建筑功能）的数据，住宅房屋的价值占比有何变化趋势？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:11<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[26/118] 如何分析广联达在“两新一重”建设政策下的业绩表现？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:20<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27/118] 如何分析广联达在建筑业数字化转型中的竞争优势？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:23<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28/118] 如何分析广联达在数字化转型中的增长潜力？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:41<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29/118] 广联达的数字施工业务在2020年的资金压力如何？与同行业其他企业相比，其资金压力有何特点？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:41<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30/118] 广联达的施工总承包资质分为几个类别？每个类别分别对应哪些工程类型？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:50<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31/118] 如何分析广联达施工总承包特级资质对工程施工企业的通用要求？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:53<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32/118] 如何评估广联达在数字化转型过程中面临的挑战及其应对策略？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [01:54<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33/118] 如何评估广联达在数字化转型过程中面临的项目超期与成本超支问题？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:06<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34/118] 如何评估广联达在数字化转型过程中面临的安全事故和质量问题？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:12<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[35/118] 广联达：根据图表64，哪些技术被认为对未来建筑业发展影响最大？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:13<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36/118] 如何评估广联达在数字化转型中的竞争优势？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:22<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37/118] 如何评估广联达在5G和云计算技术应用下的发展潜力？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:26<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38/118] 广联达：如何看待其数字孪生业务的发展前景？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:31<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39/118] 广联达的数字化转型政策有哪些主要内容？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:41<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40/118] 广联达：如何通过EPC模式推进建筑业数字化转型？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:44<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[41/118] 广联达：如何看待其数字造价业务的增长潜力？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:48<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42/118] 广联达：根据图表88，PC装配式建筑相较于现场浇筑建筑在哪些方面表现出优势？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [02:53<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43/118] 广联达：在双碳政策背景下，公司如何通过数字化手段优化资源管理并应对绿色建筑挑战？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:05<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44/118] 广联达的数字造价业务未来成长空间是多少？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:07<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[45/118] 如何分析广联达在数字施工业务上的增长潜力？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:15<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46/118] 如何分析广联达云造价业务的营收增长趋势及其与房屋新开工面积增速的关系？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:20<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[47/118] 广联达的造价业务云转型进展如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:22<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48/118] 广联达：根据图表124和125的数据，2020年中国各行业建筑总产值和工程造价咨询业务收入结构分别呈现怎样的分布？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:28<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[49/118] 广联达：根据图表130和131，预测2025年我国电子招标采购交易规模将达到多少亿元？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:34<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50/118] 如何评价广联达在公共资源交易领域的“5+3+5”智慧交易应用体系？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:41<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51/118] 如何评估广联达在数字施工业务上的市场潜力和发展前景？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:44<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52/118] 如何评估广联达在数字化转型中的竞争优势及其未来发展前景？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [03:53<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[53/118] 关于广联达的“114N”数字项目管理平台，请问该平台的主要理念、平台、技术及应用分别是什么？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:02<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[54/118] 关于广联达的CAD快速看图工具及其七大功能价值，请问这些功能如何具体提升用户的工作效率？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:02<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[55/118] 如何评价广联达在搅拌站材料核算系统上的核心功能及其工作流程？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:04<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56/118] 关于广联达公司的AECORE平台及其九大服务产品模块，请问如何评估这些服务模块在推动建筑行业数字化转型方面的具体作用？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:17<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57/118] 关于广联达公司的AECORE四大云解决方案，能否详细解释一下设计云和施工云的主要功能？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:22<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58/118] 如何评估广联达在2019年至2021年上半年期间，施工业务的客户覆盖率和渗透率的变化趋势？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:24<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59/118] 如何评估广联达在数字化转型中的竞争优势及其未来增长潜力？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:29<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[60/118] 广联达：根据图表201，中国头部工程设计企业在设计业务的海外营收占比与全球工程设计前10强企业相比有何差异？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:35<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[61/118] 关于广联达公司的工程设计业务，其在中国工程设计企业数量占比情况如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:43<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62/118] 关于广联达公司的设计业务营收占比趋势及其影响因素，请问您能否详细分析？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:54<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63/118] 广联达：2018-2019年建筑设计业务的毛利率变化趋势如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [04:57<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64/118] 广联达：根据Autodesk的预测，2026年中国工程设计软件市场规模约为多少？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [05:01<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65/118] 广联达：根据图表217和218，您能否分析一下国家在支持工业软件和建筑业软件系统自主可控方面的政策趋势？...\n",
      "[66/118] 如何评价广联达在数字设计业务上的“收购+自研”布局策略？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [05:15<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[67/118] 关于广联达的数维设计产品集及其运行架构，请问如何评价其在数字化集成设计环境中的优势？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [05:27<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[68/118] 关于广联达（002410.SZ）的盈利预测，能否详细解释一下公司施工业务毛利率较低的原因及其对公司整体盈利的影响？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [05:28<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69/118] 根据广联达（002410.SZ）的财务数据，请分析其2021年至2023年的盈利预测与估值情况，并说明维持“买入”评级的...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [05:33<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70/118] 如何分析广联达（002410.SZ）在2021年的PS估值水平及其与可比公司的差异？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [05:41<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71/118] 千味央厨的营收结构如何随时间变化？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [05:43<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72/118] 千味央厨的股价走势如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [05:59<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73/118] 千味央厨的速冻米面制品产品结构与日本相比有何差异？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [06:03<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74/118] 千味央厨的2021年国内零售端速冻米面市场竞争格局如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [06:06<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75/118] 千味央厨的业务模式如何帮助公司在大客户资源方面取得竞争优势？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [06:15<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[76/118] 千味央厨 (001215 CH) 的速冻米面业务在餐饮端的增长潜力如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [06:26<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[77/118] 千味央厨：根据图表20和21的数据，能否分析日本餐饮及零售端速冻食品销量的变化趋势及其背后的主要驱动因素？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [06:32<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[78/118] 根据华泰证券发布的《千味央厨-千寻百味乘势而上》报告，结合图表23和24的数据，请问2025年国内餐饮端速冻米面市场的潜...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [06:38<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79/118] 千味央厨的供应链管理体系如何保障产品质量和供应稳定性？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [06:44<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[80/118] 千味央厨的模拟后厨实验室是如何具体运作的？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [06:53<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[81/118] 千味央厨的供应链管理策略如何影响其成本控制和产品品质？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [07:02<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[82/118] 千味央厨的餐饮大客户经营数据在2022年第三季度有何变化？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [07:03<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83/118] 千味央厨的经销渠道策略如何影响其中小客户开拓？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [07:20<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[84/118] 千味央厨的三大新品牌分别针对哪些消费群体？各自的产品特点是什么？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [07:38<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[85/118] 郑州千味央厨食品股份有限公司在发行前后的股权结构发生了哪些变化？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [07:41<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86/118] 千味央厨公司在速冻面米市场占有率如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [07:43<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87/118] 千味央厨公司在2020年的毛利率受原材料价格波动影响如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [08:02<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[88/118] 千味央厨2020年的原材料储备占比存货显著提升的具体情况如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [08:06<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[89/118] 千味央厨公司募投项目的具体投资金额是多少？...\n",
      "[90/118] 针对千味央厨的募投项目，其总部基地及研发中心建设的具体投资内容和占比是多少？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [08:12<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[91/118] 根据千味央厨的招股书，其2021年的收入预测是多少？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [08:34<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92/118] 千味央厨：使用OEM代工模式的优势体现在哪些方面？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [08:55<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93/118] 千味央厨的专利布局对其研发能力有何影响？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [08:57<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[94/118] 千味央厨：2020年五大直营客户销售额及占比情况如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [09:02<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[95/118] 关于伊利股份的历史发展和市场竞争，请问在2005年至2013年间，伊利如何通过创新产品和营销策略实现营收突破100亿大关...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [09:15<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96/118] 关于伊利股份（600887）的深度报告《王者荣耀，行稳致远》，请问在2009年之后，伊利是如何通过营销策略和产品创新实现...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [09:20<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97/118] 关于伊利股份的奶源布局和产能分布，请问如何通过图表分析伊利在国内和国外的奶源和产能布局？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [09:41<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98/118] 如何分析伊利股份在产品研发方面的策略及其对业绩的影响？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [09:43<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99/118] 根据伊利股份的公司深度报告《王者荣耀行稳致远》，请问伊利在2005年后通过哪些营销策略实现了后来居上？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [09:44<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100/118] 根据《伊利股份-公司深度报告》中的内容，请分析影响奶粉行业规模的主要因素，并解释为什么我国奶粉单价高于多个发达国家？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [09:45<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101/118] 根据《伊利股份-公司深度报告》中的内容，请问伊利在母乳研究方面有哪些重要成就？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [10:07<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102/118] 关于伊利股份的奶酪行业分析，您能否详细解释一下奶酪在中国市场的增长潜力以及与国际市场相比的差距？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [10:07<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103/118] 伊利股份的奶酪业务发展现状如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [10:10<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104/118] 广联达（002410）的数字建筑平台战略如何实现软硬一体化？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [10:14<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105/118] 广联达（002410）的施工数字化转型一站式服务如何具体实现？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   1%|          | 1/118 [10:34<1:23:37, 42.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106/118] 广联达（002410）在2015年至2018年间，通过哪些具体措施推动了“数字企业+智慧工地+BIM建造”的发展？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   8%|▊         | 9/118 [10:37<2:10:20, 71.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107/118] 根据广联达（002410）2021年8月23日发布的深度跟踪报告，结合图片中的数据，请问广联达在2020年的施工业务合同...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   9%|▉         | 11/118 [10:50<1:39:22, 55.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[108/118] 广联达（002410）在数字建筑一体化方面有哪些具体解决方案？...\n",
      "[109/118] 广联达的数字企业业务在2019年至2021年间发生了哪些变化？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:   9%|▉         | 11/118 [10:51<1:39:22, 55.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[110/118] 广联达（002410）：BIM5D+智慧工地平台在施工业务中的应用现状如何？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:  11%|█         | 13/118 [10:57<1:13:36, 42.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[111/118] 广联达在人员管理数字化方面有哪些具体措施？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:  11%|█         | 13/118 [11:05<1:13:36, 42.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112/118] 广联达在数字建筑一体化领域采取了哪些具体的落地打法？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:  11%|█         | 13/118 [11:12<1:13:36, 42.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113/118] 根据广联达（002410）的深度跟踪报告，未来公司在智慧工地和BIM技术方面的布局将如何影响其长期收入目标的实现？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:  11%|█         | 13/118 [11:20<1:13:36, 42.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[114/118] 广联达（002410）在2021年云收入占比达到多少？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:  11%|█         | 13/118 [11:24<1:13:36, 42.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[115/118] 广联达（002410）的数字设计业务在2021年下半年将如何推进？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:  11%|█         | 13/118 [11:28<1:13:36, 42.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[116/118] 广联达（002410）在数字建筑一体化方面有哪些具体的产品定位和解决方案？...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:  11%|█         | 13/118 [11:33<1:13:36, 42.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[117/118] 根据广联达（002410）的利润表和资产负债表数据，请分析其未来三年的毛利率趋势及其影响因素。...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample:  11%|█         | 13/118 [11:35<1:13:36, 42.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118/118] 根据广联达（002410）的现金流量表和主要财务指标，请分析其未来三年的经营现金流趋势及其对公司盈利能力的影响。...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Infer on train sample: 100%|██████████| 118/118 [15:55<00:00,  8.10s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw predictions to: d:\\Datawhale\\Multimodal-RAG-Competitions\\eval_train_raw.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 5) Inference\n",
    "def run_one(idx):\n",
    "    item = train_data[idx]\n",
    "    q = item.get(\"question\", \"\")\n",
    "    tqdm.write(f\"[{sample_idx.index(idx)+1}/{len(sample_idx)}] {q[:60]}...\")\n",
    "    pred = rag.generate_answer(q, top_k=5)\n",
    "    return idx, pred\n",
    "\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as ex:\n",
    "    for out in tqdm(ex.map(run_one, sample_idx), total=len(sample_idx), desc=\"Infer on train sample\"):\n",
    "        results.append(out)\n",
    "\n",
    "# Save raw (idx, pred) for debugging\n",
    "with open(EVAL_RAW_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved raw predictions to: {EVAL_RAW_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26f5f2",
   "metadata": {},
   "source": [
    "## 6) Scoring vs Ground Truth\n",
    "Score per item:\n",
    "- page_match: 1 if exact page equals, else 0 (×0.25)\n",
    "- filename_match: 1 if exact filename equals, else 0 (×0.25)\n",
    "- answer_jaccard: char Jaccard (×0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1de5ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scored results to: d:\\Datawhale\\Multimodal-RAG-Competitions\\eval_train_scored.json\n",
      "max score: 0.9333333333333333\n",
      "Mean score: 0.3841\n",
      "min score: 0.056338028169014086\n",
      "Mean Jaccard: 0.3020\n",
      "Filename exact@1: 0.5339\n",
      "Page exact@1: 0.3983\n"
     ]
    }
   ],
   "source": [
    "# 6) Score predictions\n",
    "idx2pred = {idx: pred for idx, pred in results}\n",
    "\n",
    "scored_rows = []\n",
    "for idx in sample_idx:\n",
    "    gt = train_data[idx]\n",
    "    pred = idx2pred.get(idx, {})\n",
    "\n",
    "    gt_q = gt.get(\"question\", \"\")\n",
    "    gt_a = gt.get(\"answer\", \"\")\n",
    "    gt_f = gt.get(\"filename\", \"\")\n",
    "    gt_p = gt.get(\"page\", \"\")\n",
    "\n",
    "    pr_a = pred.get(\"answer\", \"\")\n",
    "    pr_f = pred.get(\"filename\", \"\")\n",
    "    pr_p = pred.get(\"page\", \"\")\n",
    "\n",
    "    page_match = 1.0 if str(pr_p) == str(gt_p) else 0.0\n",
    "    filename_match = 1.0 if str(pr_f) == str(gt_f) else 0.0\n",
    "    answer_sim = jaccard_char(str(pr_a), str(gt_a))\n",
    "\n",
    "    score = 0.25 * page_match + 0.25 * filename_match + 0.5 * answer_sim\n",
    "\n",
    "    scored_rows.append({\n",
    "        \"idx\": idx,\n",
    "        \"question\": gt_q,\n",
    "        \"gt_answer\": gt_a,\n",
    "        \"pr_answer\": pr_a,\n",
    "        \"gt_filename\": gt_f,\n",
    "        \"pr_filename\": pr_f,\n",
    "        \"gt_page\": gt_p,\n",
    "        \"pr_page\": pr_p,\n",
    "        \"page_match\": page_match,\n",
    "        \"filename_match\": filename_match,\n",
    "        \"answer_jaccard\": answer_sim,\n",
    "        \"score\": score,\n",
    "    })\n",
    "\n",
    "# Sort by score ascending to inspect weak cases first\n",
    "scored_rows_sorted = sorted(scored_rows, key=lambda x: x[\"score\"])\n",
    "\n",
    "with open(EVAL_SUMMARY_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scored_rows_sorted, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved scored results to: {EVAL_SUMMARY_PATH}\")\n",
    "print(f\"max score: {max(r['score'] for r in scored_rows_sorted)}\")\n",
    "print(f\"Mean score: {sum(r['score'] for r in scored_rows_sorted)/len(scored_rows_sorted):.4f}\")\n",
    "print(f\"min score: {min(r['score'] for r in scored_rows_sorted) }\")\n",
    "print(f\"Mean Jaccard: {sum(r['answer_jaccard'] for r in scored_rows_sorted)/len(scored_rows_sorted):.4f}\")\n",
    "print(f\"Filename exact@1: {sum(r['filename_match'] for r in scored_rows_sorted)/len(scored_rows_sorted):.4f}\")\n",
    "print(f\"Page exact@1: {sum(r['page_match'] for r in scored_rows_sorted)/len(scored_rows_sorted):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ec94c",
   "metadata": {},
   "source": [
    "max score: 0.8116883116883117  \n",
    "Mean score: 0.3678  \n",
    "min score: 0.06818181818181818  \n",
    "Mean Jaccard: 0.3118  \n",
    "Filename exact@1: 0.4407  \n",
    "Page exact@1: 0.4068  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b900b85",
   "metadata": {},
   "source": [
    "Saved scored results to: d:\\Datawhale\\Multimodal-RAG-Competitions\\eval_train_scored.json  \n",
    "max score: 0.5576923076923077  \n",
    "Mean score: 0.3201  \n",
    "min score: 0.08403361344537816  \n",
    "Mean Jaccard: 0.3068  \n",
    "Filename exact@1: 0.5833  \n",
    "Page exact@1: 0.0833  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e73f091",
   "metadata": {},
   "source": [
    "Mineru + BN25\n",
    "max score: 0.4962686567164179  \n",
    "Mean score: 0.3231  \n",
    "min score: 0.09130434782608696  \n",
    "Mean Jaccard: 0.3128\n",
    "Filename exact@1: 0.5833  \n",
    "Page exact@1: 0.0833  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9995b2d",
   "metadata": {},
   "source": [
    "Note: With BM25 rerank :\n",
    "max score: 0.5732323232323233  \n",
    "Mean score: 0.2638   \n",
    "min score: 0.09668508287292818  \n",
    "Mean Jaccard: 0.3193  \n",
    "Filename exact@1: 0.4167  \n",
    "Page exact@1: 0.0000  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7fda451",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "— Worst cases —\n",
      "\n",
      "Score: 0.056338028169014086\n",
      "Q: 千味央厨公司募投项目的具体投资金额是多少？\n",
      "GT: 根据图片中的表格信息，千味央厨公司募投项目的具体投资金额如下：\n",
      "\n",
      "- 新乡千味央厨食品有限公司食品加工建设项目（三期）的投资总额为37682.39万元。\n",
      "- 总部基地及研发中心建设项目的投资总额为5979.98万元。\n",
      "\n",
      "因此，千味央厨公司募投项目的具体投资金额总计为43662.37万元。\n",
      "PR: 43,662.37万元\n",
      "GT file/page: 千味央厨-厨之道央泱食材中有千味结-21090631页.pdf 28\n",
      "PR file/page: 千味央厨-数十载深耕速冻米面洞察需求定位B端-21090931页.pdf 27\n",
      "\n",
      "Score: 0.06040268456375839\n",
      "Q: 广联达：如何看待其数字孪生业务的发展前景？\n",
      "GT: 根据图片中的内容，广联达如何看待其数字孪生业务的发展前景可以从以下几个方面进行分析：\n",
      "\n",
      "1. **数字孪生的概念**：\n",
      "   - 数字孪生是一个充分利用物理模型、传感器更新、运行历史等数据，集成多学科、多物理量、多尺度、多概率的仿真过程，在虚拟空间中完成映射，从而反映相对应的实体装备的全生命周期过程。\n",
      "   - 它是在一个设备或系统的基础上，创造一个数字版的“克隆体”，创造出来的数字孪生体，是现实世界中实体的动态仿真。\n",
      "\n",
      "2. **发展历程**\n",
      "PR: 检索内容未直接提及数字孪生业务，但提及数字设计相关布局及施工业务发展，可参考数维设计体系与智慧工地解决方案。\n",
      "GT file/page: 广联达-再谈广联达当前时点下如何看待其三条增长曲线-220217131页.pdf 46\n",
      "PR file/page: 广联达-跟踪报告数字建筑龙头五问五答-21120815页.pdf 3\n",
      "\n",
      "Score: 0.06201550387596899\n",
      "Q: 广联达在人员管理数字化方面有哪些具体措施？\n",
      "GT: 广联达在人员管理数字化方面的具体措施包括：\n",
      "\n",
      "1. 通过无感考勤，解决用工结算及工作留痕问题。\n",
      "2. 场内定位：获取工人在作业部位停留的数据，形成工人的区域轨迹，关联施工工序形成更加准确的工效分析。\n",
      "3. 危险区域语音警告。\n",
      "PR: 广联达通过校招提升院校门槛并培养潜在人才生态，社招建立分类培养体系（如TOT、AOA、BOB等）及人才地图，同时通过股权激励计划提升员工积极性，推动人才供应链和数字教育联盟建设。\n",
      "GT file/page: 广联达-深度跟踪报告数字建筑一体化领军-21082339页.pdf 15\n",
      "PR file/page: 广联达-二次创业关键节点短中长期看好公司价值-21102062页.pdf 49\n",
      "\n",
      "Score: 0.08823529411764706\n",
      "Q: 广联达（002410）的数字设计业务在2021年下半年将如何推进？\n",
      "GT: 根据图片中的信息，广联达（002410）的数字设计业务在2021年下半年将计划发布广联达设计品牌。\n",
      "PR: 2021年下半年广联达重点落实交付保障收入与回款，并执行大客户战略以确保回款，同时持续拓展客户和项目数量。\n",
      "GT file/page: 广联达-深度跟踪报告数字建筑一体化领军-21082339页.pdf 27\n",
      "PR file/page: 广联达-深度报告造价施工设计业务齐发力打造数字建筑平台服务龙头-2022021133页.pdf 30\n",
      "\n",
      "Score: 0.09375\n",
      "Q: 根据广联达（002410）的深度跟踪报告，未来公司在智慧工地和BIM技术方面的布局将如何影响其长期收入目标的实现？\n",
      "GT: 根据广联达（002410）的深度跟踪报告，未来公司在智慧工地和BIM技术方面的布局将对其长期收入目标的实现产生以下影响：\n",
      "\n",
      "1. **短期看智慧工地**：\n",
      "   - 报告指出，智慧工地的发展正当时，且拐点正逐步显现。这表明智慧工地的应用正在加速，未来几年内有望带来显著的增长。\n",
      "\n",
      "2. **中期看BIM技术**：\n",
      "   - 随着数字化程度加深，建筑行业向平台化模式发展，一体化协同有望成为建筑信息化未来趋势。BIM技术作为其中的关键技术之一\n",
      "PR: 智慧工地和BIM技术布局通过产品结构优化（软件占比提升）及全流程覆盖增强议价能力，预计推动毛利率上升并拓展营收来源，从而支撑长期收入目标实现。\n",
      "GT file/page: 广联达-深度跟踪报告数字建筑一体化领军-21082339页.pdf 19\n",
      "PR file/page: 广联达-跟踪报告数字建筑龙头五问五答-21120815页.pdf 9\n",
      "\n",
      "Score: 0.09401709401709402\n",
      "Q: 广联达的造价业务云转型进展如何？\n",
      "GT: 广联达的造价业务云转型进展如下：\n",
      "\n",
      "1. **云转型效果持续超预期**：自2017年起正式开启造价业务云转型以来，公司按照分地区、分产品的思路逐步推进转型。目前，造价业务云转型进入了最后阶段，绝大部分地区已经启动或完成初步转型，转化率与续费率基本均超过了80%。\n",
      "\n",
      "2. **2021年云转型第一阶段基本完成**：2021年公司开启了最后四个地区（江苏、浙江、福建、安徽）的云转型，21H1即实现四地区收入正\n",
      "PR: 2020年上半年广联达造价业务云订阅服务收入突破50%，云转型深入推动盈利水平持续好转。\n",
      "GT file/page: 广联达-再谈广联达当前时点下如何看待其三条增长曲线-220217131页.pdf 60\n",
      "PR file/page: 公司研究广联达-首次覆盖造价施工业务为基向建筑信息化龙头平台迈进-2020093033页.pdf 16\n",
      "\n",
      "— Best cases —\n",
      "\n",
      "Score: 0.7584269662921348\n",
      "Q: 广联达的股权激励措施对公司的人效和薪酬有何影响？\n",
      "GT: 广联达的股权激励措施对公司的人效和薪酬产生了积极的影响：\n",
      "\n",
      "1. **人效提升**：\n",
      "   - 在高待遇的激励下，公司的员工人效水平持续提升。以还原后收入口径计算，公司人均创收从2011年的25.07万元增至2020年的56.21万元。\n",
      "\n",
      "2. **薪酬增加**：\n",
      "   - 自2012年以来，公司员工的平均薪酬从2012年的14.63万元增至2020年的32.25万元。这表明公司通过股权\n",
      "PR: 广联达的股权激励措施显著提升了员工薪酬和人效。员工平均薪酬从2012年的14.63万元增至2020年的32.25万元，人均创收从25.07万元增至56.21万元，显示高激励政策有效增强了员工积极性和生产效率。\n",
      "GT file/page: 广联达-再谈广联达当前时点下如何看待其三条增长曲线-220217131页.pdf 16\n",
      "PR file/page: 广联达-再谈广联达当前时点下如何看待其三条增长曲线-220217131页.pdf 16\n",
      "\n",
      "Score: 0.7682926829268293\n",
      "Q: 极兔速递-W：根据图表分析，2023年东南亚各国电商GMV增速最快的国家是哪一个？\n",
      "GT: 根据图片中的文字内容，2023年东南亚各国电商GMV增速最快的国家是越南，其同比增速为52.9%。\n",
      "PR: 2023年东南亚各国电商GMV增速最快的国家是越南\n",
      "GT file/page: 极兔速递W-港股公司研究报告-系列一东南亚十年磨砺终成锋产业经营双拐点-25070834页.pdf 6\n",
      "PR file/page: 极兔速递W-港股公司研究报告-系列一东南亚十年磨砺终成锋产业经营双拐点-25070834页.pdf 6\n",
      "\n",
      "Score: 0.7916666666666667\n",
      "Q: 联邦制药的TUL01101片剂和软膏剂在特应性皮炎治疗中的疗效如何？\n",
      "GT: 联邦制药的TUL01101片剂和软膏剂在特应性皮炎治疗中的疗效显著，安全性良好。具体表现为：\n",
      "\n",
      "1. **临床结果**：\n",
      "   - Ia期和Ib期临床结果显示，TUL01101在人体内有着良好的量效关系。\n",
      "   - 在患者中疗效显著。\n",
      "\n",
      "2. **安全性**：\n",
      "   - 安全性良好。\n",
      "\n",
      "3. **依从性**：\n",
      "   - 相较注射剂型的度普利尤单抗，TUL01101具备更好的依从性。\n",
      "\n",
      "4\n",
      "PR: 联邦制药的TUL01101片剂和软膏在特应性皮炎治疗中表现出良好的量效关系，疗效显著且安全性良好，依从性优于注射剂型的度普利尤单抗。\n",
      "GT file/page: 联邦制药-港股公司研究报告-创新突破三靶点战略联姻诺和诺德-25071225页.pdf 12\n",
      "PR file/page: 联邦制药-港股公司研究报告-创新突破三靶点战略联姻诺和诺德-25071225页.pdf 12\n",
      "\n",
      "Score: 0.8020833333333333\n",
      "Q: 极兔速递-W：2024年单位运输成本和分拣成本分别下降了多少百分比？\n",
      "GT: 根据图片中的图表和文字内容：\n",
      "\n",
      "- 图41显示，2024年极兔单位运输成本同比下降了11.8%。\n",
      "- 图42显示，2024年极兔单位分拣成本同比下降了44.4%。\n",
      "\n",
      "因此，2024年极兔速递-W的单位运输成本下降了11.8%，单位分拣成本下降了44.4%。\n",
      "PR: 2024年极兔速递-W的单位运输成本同比下降11.8%，单位分拣成本同比下降44.4%。\n",
      "GT file/page: 极兔速递W-港股公司研究报告-系列一东南亚十年磨砺终成锋产业经营双拐点-25070834页.pdf 21\n",
      "PR file/page: 极兔速递W-港股公司研究报告-系列一东南亚十年磨砺终成锋产业经营双拐点-25070834页.pdf 21\n",
      "\n",
      "Score: 0.8118811881188119\n",
      "Q: 联邦制药的UBT37034在超重/肥胖适应症方面取得了哪些临床前数据？\n",
      "GT: 根据图片中的文字内容，联邦制药的UBT37034在超重/肥胖适应症方面的临床前数据如下：\n",
      "\n",
      "1. UBT37034在饮食诱导肥胖大鼠（DIO Rats）上的临床前数据表明，21天给药后，UBT37034联用替尔泊肽减重13.6%。\n",
      "\n",
      "2. 相比之下，Petrelintide联用替尔泊肽减重13.6%（-9.38%），Cagrilintide联用替尔泊肽减\n",
      "PR: 联邦制药的UBT37034在饮食诱导肥胖大鼠（DIO Rats）临床前试验中，联用替尔泊肽21天后实现13.6%减重，优于Petrelintide联用替尔泊肽（-9.38%）、Cagrilintide联用替尔泊肽（-10.89%）及替尔泊肽单药（-3.02%）的效果。\n",
      "GT file/page: 联邦制药-港股公司研究报告-创新突破三靶点战略联姻诺和诺德-25071225页.pdf 11\n",
      "PR file/page: 联邦制药-港股公司研究报告-创新突破三靶点战略联姻诺和诺德-25071225页.pdf 11\n",
      "\n",
      "Score: 0.9333333333333333\n",
      "Q: 广联达：根据Autodesk的预测，2026年中国工程设计软件市场规模约为多少？\n",
      "GT: 根据Autodesk的预测，2026年中国工程设计软件市场规模约为48亿美元，即约300亿元。\n",
      "PR: 根据Autodesk预测，2026年中国工程设计软件市场潜在规模约为48亿美元，即约300亿元人民币。\n",
      "GT file/page: 广联达-再谈广联达当前时点下如何看待其三条增长曲线-220217131页.pdf 113\n",
      "PR file/page: 广联达-再谈广联达当前时点下如何看待其三条增长曲线-220217131页.pdf 113\n"
     ]
    }
   ],
   "source": [
    "# Show a couple of worst and best cases inline (adjust k as needed)\n",
    "k = 6\n",
    "print(\"— Worst cases —\")\n",
    "for r in scored_rows_sorted[:k]:\n",
    "    print(\"\\nScore:\", r[\"score\"])\n",
    "    print(\"Q:\", r[\"question\"])\n",
    "    print(\"GT:\", r[\"gt_answer\"])\n",
    "    print(\"PR:\", r[\"pr_answer\"])\n",
    "    print(\"GT file/page:\", r[\"gt_filename\"], r[\"gt_page\"])\n",
    "    print(\"PR file/page:\", r[\"pr_filename\"], r[\"pr_page\"])\n",
    "\n",
    "print(\"\\n— Best cases —\")\n",
    "for r in scored_rows_sorted[-k:]:\n",
    "    print(\"\\nScore:\", r[\"score\"])\n",
    "    print(\"Q:\", r[\"question\"])\n",
    "    print(\"GT:\", r[\"gt_answer\"])\n",
    "    print(\"PR:\", r[\"pr_answer\"])\n",
    "    print(\"GT file/page:\", r[\"gt_filename\"], r[\"gt_page\"])\n",
    "    print(\"PR file/page:\", r[\"pr_filename\"], r[\"pr_page\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb39aa2",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- Set a different **sample fraction** by changing the `0.10` in `math.ceil(N * 0.10)`.\n",
    "- If `filename`/`page` in ground truth differ in minor formatting (e.g., case, spaces), add normalization before comparison.\n",
    "- You can plug this same scorer later for validation on a dev split.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
