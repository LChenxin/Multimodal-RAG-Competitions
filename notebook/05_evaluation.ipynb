{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e90ee101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 59 rows\n",
      "Keys in first row: ['answer_jaccard', 'filename_match', 'gt_answer', 'gt_filename', 'gt_page', 'idx', 'page_match', 'pr_answer', 'pr_filename', 'pr_page', 'question', 'score']\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import json, os, re, math, statistics\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "EVAL_PATH = NOTEBOOK_DIR.parent / \"eval_train_scored.json\"\n",
    "RAW_PATH = NOTEBOOK_DIR.parent / \"eval_train_raw.json\"\n",
    "\n",
    "with open(EVAL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# If file is a dict with a \"results\" field, normalize:\n",
    "if isinstance(data, dict) and \"results\" in data:\n",
    "    data = data[\"results\"]\n",
    "\n",
    "print(f\"Loaded {len(data)} rows\")\n",
    "# Peek first row keys to understand schema\n",
    "print(\"Keys in first row:\", sorted(list(data[0].keys())))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de9be5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scored rows: 59\n",
      "Scored keys example: ['answer_jaccard', 'filename_match', 'gt_answer', 'gt_filename', 'gt_page', 'idx', 'page_match', 'pr_answer', 'pr_filename', 'pr_page', 'question', 'score']\n",
      "Raw pairs: 59\n"
     ]
    }
   ],
   "source": [
    "def normalize_filename(name: str) -> str:\n",
    "    if not name: return \"\"\n",
    "    base = os.path.basename(str(name))\n",
    "    base = base.replace(\"（\",\"(\").replace(\"）\",\")\").replace(\"，\",\",\").replace(\"：\",\":\")\n",
    "    base = re.sub(r\"\\s+\", \"\", base)\n",
    "    base = re.sub(r\"(\\d+)页(?=\\.pdf$)\", \"\", base, flags=re.I)  # drop trailing \"...NN页\" before .pdf\n",
    "    return base.lower()\n",
    "\n",
    "def tokenize_zh_en(s: str):\n",
    "    s = (s or \"\").strip()\n",
    "    return re.findall(r\"[\\u4e00-\\u9fff]|[A-Za-z0-9]+\", s)\n",
    "\n",
    "def jaccard(a: str, b: str) -> float:\n",
    "    A, B = set(tokenize_zh_en(a)), set(tokenize_zh_en(b))\n",
    "    if not A and not B: return 1.0\n",
    "    if not A or not B: return 0.0\n",
    "    return len(A & B) / len(A | B)\n",
    "\n",
    "# ---- Load scored (grounded evaluation) ----\n",
    "with open(EVAL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    eval_rows = json.load(f)\n",
    "print(\"Scored rows:\", len(eval_rows))\n",
    "print(\"Scored keys example:\", sorted(eval_rows[0].keys()))\n",
    "\n",
    "eval_df = pd.DataFrame(eval_rows)\n",
    "\n",
    "# ---- Load raw (with telemetry/debug, retrieval_chunks) ----\n",
    "with open(RAW_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_rows = json.load(f)\n",
    "print(\"Raw pairs:\", len(raw_rows))\n",
    "# raw is [[idx, {...}], ...]\n",
    "raw_df = pd.DataFrame([{\"idx\": k, **v} for k, v in raw_rows])\n",
    "\n",
    "# normalize file cols for later joins\n",
    "for col in [\"gt_filename\",\"pr_filename\",\"filename\",\"model_file\",\"top_file\"]:\n",
    "    if col in eval_df.columns:\n",
    "        eval_df[col+\"_norm\"] = eval_df[col].map(normalize_filename)\n",
    "for col in [\"filename\",\"debug\"]:\n",
    "    if col in raw_df.columns and col==\"filename\":\n",
    "        raw_df[col+\"_norm\"] = raw_df[col].map(normalize_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8201d99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "N                   59.000000\n",
       "Mean score           0.396920\n",
       "Mean Jaccard         0.336214\n",
       "Filename exact@1     0.508475\n",
       "Page exact@1         0.406780\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "def acc_exact(pred, gt):\n",
    "    m = pred.notna() & gt.notna()\n",
    "    return float((pred[m]==gt[m]).mean())\n",
    "\n",
    "summary = {\n",
    "    \"N\"                 : len(eval_df),\n",
    "    \"Mean score\"        : float(eval_df[\"score\"].mean()),\n",
    "    \"Mean Jaccard\"      : float(eval_df[\"answer_jaccard\"].mean()),\n",
    "    \"Filename exact@1\"  : float(eval_df[\"filename_match\"].mean()),\n",
    "    \"Page exact@1\"      : float(eval_df[\"page_match\"].mean()),\n",
    "}\n",
    "pd.Series(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86cef936",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(bucket\n",
       " Both-wrong            29\n",
       " Both-correct          24\n",
       " File-OK/Page-Wrong     6\n",
       " Name: count, dtype: int64,\n",
       "                        score  answer_jaccard\n",
       " bucket                                      \n",
       " Both-correct        0.700651        0.401302\n",
       " Both-wrong          0.142500        0.285000\n",
       " File-OK/Page-Wrong  0.411695        0.323391)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filename/page correctness vs answer quality\n",
    "eval_df[\"bucket\"] = np.select(\n",
    "    [\n",
    "        (eval_df[\"filename_match\"]==1) & (eval_df[\"page_match\"]==1),\n",
    "        (eval_df[\"filename_match\"]==1) & (eval_df[\"page_match\"]==0),\n",
    "        (eval_df[\"filename_match\"]==0) & (eval_df[\"page_match\"]==1),\n",
    "    ],\n",
    "    [\"Both-correct\",\"File-OK/Page-Wrong\",\"File-Wrong/Page-OK\"],\n",
    "    default=\"Both-wrong\"\n",
    ")\n",
    "eval_df[\"bucket\"].value_counts(), eval_df.groupby(\"bucket\")[[\"score\",\"answer_jaccard\"]].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3e804b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both-wrong N: 29\n"
     ]
    }
   ],
   "source": [
    "# %% Setup\n",
    "import json, re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from difflib import SequenceMatcher\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "BASE = Path.cwd().parent  # adjust if needed\n",
    "SCORED = BASE / \"eval_train_scored.json\"\n",
    "RAW    = BASE / \"eval_train_raw.json\"  # optional, only if you want rerank telemetry\n",
    "\n",
    "scored = json.loads(SCORED.read_text(encoding=\"utf-8\"))\n",
    "if isinstance(scored, dict) and \"results\" in scored:  # normalize if wrapped\n",
    "    scored = scored[\"results\"]\n",
    "df = pd.DataFrame(scored)\n",
    "\n",
    "# Keep only Both-wrong\n",
    "bw = df[(df[\"filename_match\"]==0) & (df[\"page_match\"]==0)].copy()\n",
    "print(\"Both-wrong N:\", len(bw))\n",
    "\n",
    "# Optional: merge rerank telemetry from raw (idx -> debug)\n",
    "try:\n",
    "    raw = json.loads(RAW.read_text(encoding=\"utf-8\"))\n",
    "    raw_map = {idx: payload for idx, payload in raw}  # raw is [[idx, payload], ...]\n",
    "    def get_dbg(idx, key, default=None):\n",
    "        d = raw_map.get(idx, {}).get(\"debug\", {})\n",
    "        return d.get(key, default)\n",
    "    bw[\"top_rr\"]   = bw[\"idx\"].apply(lambda i: get_dbg(i, \"rerank_ok\", False) and get_dbg(i, \"top_rr\", None))\n",
    "    bw[\"top_file\"] = bw[\"idx\"].apply(lambda i: get_dbg(i, \"top_file\", \"\"))\n",
    "    bw[\"top_page\"] = bw[\"idx\"].apply(lambda i: get_dbg(i, \"top_page\", \"\"))\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0326df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_TOKEN_RE = re.compile(r'([一-龥A-Za-z0-9]+)')\n",
    "YEAR_RE = re.compile(r'20\\d{2}')\n",
    "\n",
    "def norm(s): \n",
    "    return (s or \"\").strip()\n",
    "\n",
    "def canonicalize_filename(fn: str):\n",
    "    \"\"\"去掉日期/页数字样板，只保留‘根标题’当作 doc_id。\"\"\"\n",
    "    x = norm(fn)\n",
    "    x = re.sub(r'[-_（(]?\\d{6,8}[)）]?页?', '', x)   # 页数\n",
    "    x = re.sub(r'[-_（(]?(19|20)\\d{2}[\\d\\-_.]*[)）]?', '', x)  # 年月日\n",
    "    x = re.sub(r'\\.pdf$', '', x, flags=re.I)\n",
    "    x = re.sub(r'\\s+', '', x)\n",
    "    return x\n",
    "\n",
    "def extract_years(s):\n",
    "    return YEAR_RE.findall(s or \"\")\n",
    "\n",
    "def company_tokens_from_filename(fn: str):\n",
    "    \"\"\"从文件名切中文/字母数字 token；你也可以换成自有公司词库匹配。\"\"\"\n",
    "    core = canonicalize_filename(fn)\n",
    "    toks = [t for t in COMPANY_TOKEN_RE.findall(core) if len(t)>=2]\n",
    "    return set(toks)\n",
    "\n",
    "def same_company_heur(pr_fn, gt_fn):\n",
    "    \"\"\"粗粒度判断是否同一家公司/系列：根ID相似 + token 重叠。\"\"\"\n",
    "    a, b = canonicalize_filename(pr_fn), canonicalize_filename(gt_fn)\n",
    "    ratio = SequenceMatcher(None, a, b).ratio()\n",
    "    ia = company_tokens_from_filename(pr_fn)\n",
    "    ib = company_tokens_from_filename(gt_fn)\n",
    "    overlap = len(ia & ib)\n",
    "    return ratio >= 0.65 or overlap >= 3, ratio, overlap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b34980af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reason\n",
      "Cross-company confusion    29\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def classify_reason(row):\n",
    "    pr, gt = row[\"pr_filename\"], row[\"gt_filename\"]\n",
    "    same_series, ratio, overlap = same_company_heur(pr, gt)\n",
    "    pr_years = extract_years(pr)\n",
    "    gt_years = extract_years(gt)\n",
    "\n",
    "    if same_series:\n",
    "        if set(pr_years) and set(gt_years) and set(pr_years) != set(gt_years):\n",
    "            return \"Edition/Year mismatch\"\n",
    "        return \"Same-series mismatch\"\n",
    "    else:\n",
    "        # 粗略：如果共有行业词（“数字化”，“数据中心”，“白酒”等）但公司词不重叠\n",
    "        pr_tok = company_tokens_from_filename(pr)\n",
    "        gt_tok = company_tokens_from_filename(gt)\n",
    "        if len(pr_tok & gt_tok) <= 1:\n",
    "            return \"Cross-company confusion\"\n",
    "        return \"Generic keyword bias\"\n",
    "\n",
    "bw[\"reason\"] = bw.apply(classify_reason, axis=1)\n",
    "print(bw[\"reason\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8de7d57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_has_year               False  True  All\n",
      "reason                                   \n",
      "Cross-company confusion     20     9   29\n",
      "All                         20     9   29\n",
      "year_mismatch            False  True  All\n",
      "reason                                   \n",
      "Cross-company confusion     15    14   29\n",
      "All                         15    14   29\n",
      "High-confidence wrong (top_rr>=0.9) by reason:\n",
      "top_rr                   False  All\n",
      "reason                             \n",
      "Cross-company confusion     29   29\n",
      "All                         29   29\n"
     ]
    }
   ],
   "source": [
    "# 问题里是否出现年份，而 PR/GT 年份不一致？\n",
    "def year_in_question(q):\n",
    "    return bool(YEAR_RE.search(q or \"\"))\n",
    "\n",
    "bw[\"q_has_year\"] = bw[\"question\"].apply(year_in_question)\n",
    "\n",
    "def years_set(s): \n",
    "    return set(extract_years(s))\n",
    "\n",
    "bw[\"pr_years\"] = bw[\"pr_filename\"].apply(years_set)\n",
    "bw[\"gt_years\"] = bw[\"gt_filename\"].apply(years_set)\n",
    "bw[\"year_mismatch\"] = bw.apply(lambda r: (r[\"pr_years\"] != r[\"gt_years\"]) and (r[\"gt_years\"] != set()), axis=1)\n",
    "\n",
    "print(pd.crosstab(bw[\"reason\"], bw[\"q_has_year\"], margins=True))\n",
    "print(pd.crosstab(bw[\"reason\"], bw[\"year_mismatch\"], margins=True))\n",
    "\n",
    "# 如果有 top_rr / rerank_score 可用，看看“高置信错配”的占比\n",
    "if \"top_rr\" in bw.columns:\n",
    "    print(\"High-confidence wrong (top_rr>=0.9) by reason:\")\n",
    "    print(pd.crosstab(bw[\"reason\"], (bw[\"top_rr\"]>=0.9), margins=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d038ad84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (eval_df[\"filename_match\"]==1) & (eval_df[\"page_match\"]==0)\n",
    "def _off1(r):\n",
    "    try:\n",
    "        return abs(int(r[\"pr_page\"])-int(r[\"gt_page\"]))==1\n",
    "    except: return False\n",
    "off1_ratio = float(mask.sum() and sum(_off1(r) for _,r in eval_df[mask].iterrows())/mask.sum())\n",
    "off1_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d822656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Telemetry rows: 59\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cand_n</th>\n",
       "      <th>ranked_n</th>\n",
       "      <th>page_vote_n</th>\n",
       "      <th>final_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>59.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.677966</td>\n",
       "      <td>1.677966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.899055</td>\n",
       "      <td>0.899055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>120.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cand_n  ranked_n  page_vote_n    final_n\n",
       "count    59.0      59.0    59.000000  59.000000\n",
       "mean    120.0     120.0     1.677966   1.677966\n",
       "std       0.0       0.0     0.899055   0.899055\n",
       "min     120.0     120.0     1.000000   1.000000\n",
       "25%     120.0     120.0     1.000000   1.000000\n",
       "50%     120.0     120.0     1.000000   1.000000\n",
       "75%     120.0     120.0     2.000000   2.000000\n",
       "max     120.0     120.0     4.000000   4.000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "# Telemetry spread\n",
    "dbg = pd.json_normalize(raw_df[\"debug\"].dropna())\n",
    "print(\"Telemetry rows:\", len(dbg))\n",
    "tele = pd.concat([raw_df[[\"idx\"]], dbg], axis=1)\n",
    "\n",
    "# How many went through rerank path?\n",
    "tele[\"path\"].value_counts(dropna=False)\n",
    "\n",
    "# Rerank HTTP health\n",
    "tele[\"rerank_http\"].value_counts(dropna=False).head(10)\n",
    "\n",
    "# Did rerank actually return a ranked list?\n",
    "tele[\"rerank_ok\"].value_counts(dropna=False)\n",
    "\n",
    "# Candidate pool sizes & final selection sizes\n",
    "tele[[\"cand_n\",\"ranked_n\",\"page_vote_n\",\"final_n\"]].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b4e704e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " Empty DataFrame\n",
       " Columns: [idx, path, rerank_ok, rerank_attempts, rerank_http, cand_n, ranked_n, page_vote_n, final_n, top_file, top_page, model_file, model_page]\n",
       " Index: [])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "mismatch = tele[\n",
    "    (tele[\"path\"]==\"rerank\")\n",
    "    & ((tele[\"top_file\"]!=tele[\"model_file\"]) | (tele[\"top_page\"]!=tele[\"model_page\"]))\n",
    "]\n",
    "mismatch.shape[0], mismatch.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9016e723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liuch\\AppData\\Local\\Temp\\ipykernel_2500\\1474967544.py:16: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merged.groupby(pd.cut(merged[\"top_rr\"], bins=[-1,0.3,0.6,0.8,0.9,1.1]))[[\"score\",\"filename_match\",\"page_match\"]].mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(top_rr\n",
       " (-1.0, 0.3]     1\n",
       " (0.3, 0.6]      0\n",
       " (0.6, 0.8]      2\n",
       " (0.8, 0.9]      2\n",
       " (0.9, 1.1]     54\n",
       " Name: count, dtype: int64,\n",
       "                 score  filename_match  page_match\n",
       " top_rr                                           \n",
       " (-1.0, 0.3]  0.158451             0.0    0.000000\n",
       " (0.3, 0.6]        NaN             NaN         NaN\n",
       " (0.6, 0.8]   0.382627             1.0    0.000000\n",
       " (0.8, 0.9]   0.492353             0.5    0.500000\n",
       " (0.9, 1.1]   0.398331             0.5    0.425926)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "merged = eval_df.merge(tele, on=\"idx\", how=\"left\", suffixes=(\"\",\"_dbg\"))\n",
    "\n",
    "# Rerank vs baseline quality\n",
    "merged[\"path\"].value_counts()\n",
    "merged.groupby(\"path\")[[\"score\",\"answer_jaccard\",\"filename_match\",\"page_match\"]].mean()\n",
    "\n",
    "# Does a higher top rerank_score imply better answers?\n",
    "def top_rerank_score(row):\n",
    "    rc = (raw_df.loc[raw_df[\"idx\"]==row[\"idx\"], \"retrieval_chunks\"].values or [None])[0]\n",
    "    if not rc: return None\n",
    "    s = rc[0].get(\"rerank_score\") if isinstance(rc, list) and rc else None\n",
    "    return s\n",
    "merged[\"top_rr\"] = merged.apply(top_rerank_score, axis=1)\n",
    "pd.cut(merged[\"top_rr\"], bins=[-1,0.3,0.6,0.8,0.9,1.1]).value_counts().sort_index(), \\\n",
    "merged.groupby(pd.cut(merged[\"top_rr\"], bins=[-1,0.3,0.6,0.8,0.9,1.1]))[[\"score\",\"filename_match\",\"page_match\"]].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c41ba5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'orig_file_exact': 0.5084745762711864, 'orig_page_exact': 0.4067796610169492, 'locked_file_exact': 0.5084745762711864, 'locked_page_exact': 0.4067796610169492, 'orig_mean_score': 0.3969204776660378, 'locked_mean_score_est': 0.43645737134538537}\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "sim = merged.copy()\n",
    "\n",
    "# use top_file/top_page when present; else keep pr_*\n",
    "sim[\"pr_file_locked_norm\"] = np.where(sim[\"top_file\"].notna(), sim[\"top_file\"].map(normalize_filename), sim[\"pr_filename\"].map(normalize_filename))\n",
    "sim[\"pr_page_locked\"]      = np.where(sim[\"top_page\"].notna(), sim[\"top_page\"], sim[\"pr_page\"])\n",
    "\n",
    "file_exact_locked = (sim[\"pr_file_locked_norm\"] == sim[\"gt_filename_norm\"]).mean()\n",
    "page_exact_locked = (pd.to_numeric(sim[\"pr_page_locked\"], errors=\"coerce\") == pd.to_numeric(sim[\"gt_page\"], errors=\"coerce\")).mean()\n",
    "\n",
    "# recompute a hypothetical score with locked file/page, same weights as your scorer (approx: add deltas keeping jaccard same)\n",
    "W_FILE, W_PAGE, W_ANS = 0.5, 0.2, 0.3\n",
    "score_locked = (\n",
    "    W_ANS * sim[\"answer_jaccard\"]\n",
    "  + W_FILE * (sim[\"pr_file_locked_norm\"] == sim[\"gt_filename_norm\"]).astype(float)\n",
    "  + W_PAGE * (pd.to_numeric(sim[\"pr_page_locked\"], errors=\"coerce\") == pd.to_numeric(sim[\"gt_page\"], errors=\"coerce\")).astype(float)\n",
    ").mean()\n",
    "\n",
    "print({\n",
    "    \"orig_file_exact\": float(merged[\"filename_match\"].mean()),\n",
    "    \"orig_page_exact\": float(merged[\"page_match\"].mean()),\n",
    "    \"locked_file_exact\": float(file_exact_locked),\n",
    "    \"locked_page_exact\": float(page_exact_locked),\n",
    "    \"orig_mean_score\": float(merged[\"score\"].mean()),\n",
    "    \"locked_mean_score_est\": float(score_locked)\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0908a299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(qtype\n",
       " Conceptual      39\n",
       " Numeric         14\n",
       " Figure/Table     6\n",
       " Name: count, dtype: int64,\n",
       "                  score  answer_jaccard  filename_match  page_match\n",
       " qtype                                                             \n",
       " Conceptual    0.428758        0.319054        0.589744    0.487179\n",
       " Figure/Table  0.563765        0.377530        0.833333    0.666667\n",
       " Numeric       0.236726        0.366308        0.142857    0.071429)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%\n",
    "def qtype(q: str):\n",
    "    q = q or \"\"\n",
    "    if re.search(r\"(图|表|图表|见图|见表)\", q): return \"Figure/Table\"\n",
    "    if re.search(r\"(\\d+%|同比|环比|增长|下降|多少|估值|PS|PE)\", q): return \"Numeric\"\n",
    "    return \"Conceptual\"\n",
    "\n",
    "eval_df[\"qtype\"] = eval_df[\"question\"].map(qtype)\n",
    "eval_df[\"qtype\"].value_counts(), eval_df.groupby(\"qtype\")[[\"score\",\"answer_jaccard\",\"filename_match\",\"page_match\"]].mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52089756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved two debug files.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Very low score but high rerank confidence\n",
    "hard_1 = merged[(merged[\"score\"]<0.15) & (merged[\"top_rr\"]>0.8)].head(30)[\n",
    "    [\"idx\",\"question\",\"score\",\"answer_jaccard\",\"filename_match\",\"page_match\",\"top_rr\",\"top_file\",\"top_page\",\"model_file\",\"model_page\",\"gt_filename\"]\n",
    "]\n",
    "\n",
    "# Filename off by small edit distance (quick heuristic: common prefix >= 10 chars)\n",
    "def long_prefix(a,b):\n",
    "    a,b=normalize_filename(a),normalize_filename(b)\n",
    "    n=min(len(a),len(b)); i=0\n",
    "    while i<n and a[i]==b[i]: i+=1\n",
    "    return i\n",
    "hard_2 = merged[(merged[\"filename_match\"]==0)].assign(\n",
    "    prefix_len=lambda d: d.apply(lambda r: long_prefix(r[\"pr_filename\"], r[\"gt_filename\"]), axis=1)\n",
    ").sort_values(\"prefix_len\", ascending=False).head(30)[[\"idx\",\"question\",\"pr_filename\",\"gt_filename\",\"prefix_len\"]]\n",
    "\n",
    "hard_1.to_json(\"hard_lowscore_highconf.json\", force_ascii=False, orient=\"records\", indent=2)\n",
    "hard_2.to_json(\"hard_fn_mismatch_prefix.json\", force_ascii=False, orient=\"records\", indent=2)\n",
    "print(\"Saved two debug files.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
